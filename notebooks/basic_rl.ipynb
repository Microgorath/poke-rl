{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Local Pokemon Showdown Server\n",
    "If on the devcontainer, this is done automatically every time the container is started. Otherwise:  \n",
    "cd into your pokemon-showdown directory  \n",
    "node pokemon-showdown start --no-security"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This needs to be run at the beginning or else testing will fail later.\n",
    "from ray.rllib.utils.framework import try_import_tf\n",
    "tf1, tf, tfv = try_import_tf()\n",
    "tf1.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time, copy\n",
    "from typing import List, Any, Union\n",
    "\n",
    "from gymnasium.spaces import Box, Space\n",
    "\n",
    "# from tabulate import tabulate\n",
    "\n",
    "from poke_env.environment.abstract_battle import AbstractBattle\n",
    "from poke_env.data import GenData\n",
    "type_chart = GenData(9).type_chart\n",
    "from poke_env.ps_client.account_configuration import AccountConfiguration, CONFIGURATION_FROM_PLAYER_COUNTER\n",
    "from poke_env.player import (\n",
    "    Gen9EnvSinglePlayer,\n",
    "    MaxBasePowerPlayer,\n",
    "    ObsType,\n",
    "    RandomPlayer,\n",
    "    SimpleHeuristicsPlayer\n",
    ")\n",
    "\n",
    "from ray.rllib.env.env_context import EnvContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def create_account_configuration(username: str, unique_ids: Union[List[Any], None] = None) -> AccountConfiguration:\n",
    "    # If no unique ids provided, create a unique id using the counter.\n",
    "    if unique_ids is None:\n",
    "        # NOTE: This is not thread-safe! Provide unique ids for multithreading / multi workers.\n",
    "        # The 's' in the resulting username indicates single thread.\n",
    "        CONFIGURATION_FROM_PLAYER_COUNTER.update([username])\n",
    "        unique_username = \"%s s %d\" % (username, CONFIGURATION_FROM_PLAYER_COUNTER[username])\n",
    "        if len(unique_username) > 18:\n",
    "            unique_username = \"%s s%d\" % (\n",
    "                username[: 18 - len(unique_username)],\n",
    "                CONFIGURATION_FROM_PLAYER_COUNTER[username],\n",
    "            )\n",
    "    else:\n",
    "        unique_ids_str = ' '.join(map(str, unique_ids))\n",
    "        unique_username = \"%s %s\" % (username, unique_ids_str)\n",
    "        unique_username = unique_username.strip()\n",
    "        if len(unique_username) > 18:\n",
    "            unique_username = \"%s %s\" % (\n",
    "                username[: 18 - len(unique_username)],\n",
    "                unique_ids_str,\n",
    "            )\n",
    "        \n",
    "    return AccountConfiguration(unique_username.strip(), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRLPlayer(Gen9EnvSinglePlayer):\n",
    "    def __init__(self, env_config: Union[EnvContext, dict]):\n",
    "        # Get the player config dict for easy access, using a deep copy so we can make changes to it.\n",
    "        player_config = copy.deepcopy(env_config.get('player_config'))\n",
    "\n",
    "        # Create list of the unique ids of the worker running this env.\n",
    "        if type(env_config) is EnvContext:\n",
    "            # The unique ids from EnvContext are the worker index, and the vector index.\n",
    "            # Worker index is index of the rollout worker that this env is running on, when there are multiple workers.\n",
    "            # Vector index is index of this env on this worker, when there are multiple envs per worker.\n",
    "            # TODO: Add trial index from env_config if it is added in future RLLib update.\n",
    "            unique_ids = [env_config.worker_index, env_config.vector_index]\n",
    "        else:\n",
    "            # Set it to None. This activates create_account_configuration's player counter.\n",
    "            unique_ids = None\n",
    "\n",
    "        # Create unique account configuration for this from username to prevent multithreading naming conflicts\n",
    "        # account_configuration should initially be None, unless this is created as opponent for self-play\n",
    "        if player_config.get('account_configuration') is None:\n",
    "            # If no username provided, make one from this class name. \n",
    "            username =  env_config.get('username')\n",
    "            if username is None:\n",
    "                username = type(self).__name__\n",
    "            # Create unique account configuration from username, worker process id, and worker env id \n",
    "            player_config['account_configuration'] = create_account_configuration(username=username, unique_ids=unique_ids)\n",
    "        \n",
    "        # if opponent class provided, instantiate it with its config.\n",
    "        opponent_class = env_config.get('opponent_class')\n",
    "        if opponent_class is not None:\n",
    "            # Get the opponent config dict for easy access, using a deep copy so we can make changes to it. This mirrors player_config.\n",
    "            opponent_config = copy.deepcopy(env_config.get('opponent_config'))\n",
    "\n",
    "            # Create unique opponent account configuration if none provided.\n",
    "            # opponent account_configuration should be None when using parallelization or multiple workers\n",
    "            if opponent_config.get('account_configuration') is None:\n",
    "                # If no opponent username provided, make one from its class name. \n",
    "                opponent_username =  env_config.get('opponent_username')\n",
    "                if opponent_username is None:\n",
    "                    opponent_username = type(opponent_class).__name__\n",
    "                # Create unique opponent account configuration from username, worker process index, and env index on this worker.\n",
    "                opponent_config['account_configuration'] = create_account_configuration(username=opponent_username, unique_ids=unique_ids)\n",
    "            # Instantiate the opponent class with opponent_config.\n",
    "            player_config['opponent'] = opponent_class(**opponent_config)\n",
    "        \n",
    "        super().__init__(**player_config)\n",
    "\n",
    "\n",
    "    def calc_reward(self, last_battle, current_battle) -> float:\n",
    "        return self.reward_computing_helper(\n",
    "            current_battle, fainted_value=2.0, hp_value=1.0, victory_value=30.0\n",
    "        )\n",
    "\n",
    "    def embed_battle(self, battle: AbstractBattle) -> ObsType:\n",
    "        # -1 indicates that the move does not have a base power\n",
    "        # or is not available\n",
    "        moves_base_power = -np.ones(4)\n",
    "        moves_dmg_multiplier = np.ones(4)\n",
    "        for i, move in enumerate(battle.available_moves):\n",
    "            moves_base_power[i] = (\n",
    "                move.base_power / 100\n",
    "            )  # Simple rescaling to facilitate learning\n",
    "            if move.type:\n",
    "                moves_dmg_multiplier[i] = move.type.damage_multiplier(\n",
    "                    battle.opponent_active_pokemon.type_1,\n",
    "                    battle.opponent_active_pokemon.type_2,\n",
    "                    type_chart=type_chart\n",
    "                )\n",
    "\n",
    "        # We count how many pokemons have fainted in each team\n",
    "        fainted_mon_team = len([mon for mon in battle.team.values() if mon.fainted]) / 6\n",
    "        fainted_mon_opponent = (\n",
    "            len([mon for mon in battle.opponent_team.values() if mon.fainted]) / 6\n",
    "        )\n",
    "\n",
    "        # Final vector with 10 components\n",
    "        final_vector = np.concatenate(\n",
    "            [\n",
    "                moves_base_power,\n",
    "                moves_dmg_multiplier,\n",
    "                [fainted_mon_team, fainted_mon_opponent],\n",
    "            ]\n",
    "        )\n",
    "        return np.float32(final_vector)\n",
    "\n",
    "    def describe_embedding(self) -> Space:\n",
    "        low = [-1, -1, -1, -1, 0, 0, 0, 0, 0, 0]\n",
    "        high = [3, 3, 3, 3, 4, 4, 4, 4, 1, 1]\n",
    "        return Box(\n",
    "            np.array(low, dtype=np.float32),\n",
    "            np.array(high, dtype=np.float32),\n",
    "            dtype=np.float32,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create config\n",
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "from ray import tune, train\n",
    "import os\n",
    "\n",
    "# This is passed to each environment (SimpleRLPlayer) during training.\n",
    "# 'player_config' is passed as a kwarg to the super().__init__() of SimpleRLPlayer's Gen9EnvSinglePlayer superclass.\n",
    "train_env_config = {\n",
    "    'username': 'tr_SimpleRL',\n",
    "    'player_config': {\n",
    "        'battle_format': \"gen9randombattle\",\n",
    "        'start_challenging': True,\n",
    "    },\n",
    "    'opponent_class': MaxBasePowerPlayer,\n",
    "    'opponent_username': 'tr_MaxBP',\n",
    "    'opponent_config': {\n",
    "        'battle_format': \"gen9randombattle\",\n",
    "    },\n",
    "}\n",
    "# This is passed to each environment (SimpleRLPlayer) during evaluation\n",
    "eval_env_config = {\n",
    "    'username': 'ev_SimpleRL',\n",
    "    'player_config': {\n",
    "        'battle_format': \"gen9randombattle\",\n",
    "        'start_challenging': True,\n",
    "    },\n",
    "    'opponent_class': MaxBasePowerPlayer,\n",
    "    'opponent_username': 'ev_MaxBP',\n",
    "    'opponent_config': {\n",
    "        'battle_format': \"gen9randombattle\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Guide to RLLib parameters: https://docs.ray.io/en/latest/rllib/rllib-training.html#common-parameters\n",
    "\n",
    "config = DQNConfig()\n",
    "config = config.environment(env = SimpleRLPlayer, env_config = train_env_config)\n",
    "# Set the framework to use. \"tf2\" for tensorflow, \"torch\" for PyTorch. Dev container is set up for Tensorflow 2.13.\n",
    "config = config.framework(framework=\"tf2\")\n",
    "config = config.resources(\n",
    "    num_cpus_for_main_process=4,\n",
    "    num_gpus=0,\n",
    ")\n",
    "config = config.learners(\n",
    "    num_learners=0,\n",
    "    # num_gpus_per_learner=0\n",
    ")\n",
    "config = config.env_runners(\n",
    "    # Number of cpus assigned to each env_runner. Does not improve sampling speed very much on its own. \n",
    "    num_cpus_per_env_runner=1,\n",
    "    # Number of workers to run environments. 0 forces rollouts onto the local worker. Each uses the above number of cpus.\n",
    "    num_env_runners=4,\n",
    "    # Number of environments on each env_runner worker, increasing this drastically improves sampling speed.\n",
    "    num_envs_per_env_runner=4,\n",
    "    # Don't cut off episodes before they finish when batching.\n",
    "    # As a result, the batch size hyperparameter acts as a minimum and batches may vary in size.\n",
    "    batch_mode=\"complete_episodes\",\n",
    "    # Validation creates environments and does not close them, causes problems.\n",
    "    # validate_env_runners_after_construction=False,\n",
    "    rollout_fragment_length=50,\n",
    "    # rollout_fragment_length=\"auto\",\n",
    "    explore=True,\n",
    "    exploration_config = {\n",
    "            \"type\": \"EpsilonGreedy\",\n",
    "            \"initial_epsilon\": 1.0,\n",
    "            \"final_epsilon\": 0.0,\n",
    "            \"epsilon_timesteps\": 80000,\n",
    "    }\n",
    ")\n",
    "# Set training hyperparameters. \n",
    "# For descriptions, see: https://docs.ray.io/en/latest/rllib/rllib-algorithms.html#deep-q-networks-dqn-rainbow-parametric-dqn\n",
    "config = config.training(\n",
    "    gamma=0.8,\n",
    "    lr = 0.0002,\n",
    "    # lr=tune.loguniform(8e-5,8e-3),\n",
    "    optimizer={\n",
    "        \"weight_decay\": 0.02,\n",
    "        # \"betas\": [0.9, 0.999] # May need tuning, this is default.\n",
    "    },\n",
    "    replay_buffer_config={\n",
    "        \"type\": \"MultiAgentPrioritizedReplayBuffer\",\n",
    "        \"capacity\": 100000,\n",
    "    },\n",
    "    num_steps_sampled_before_learning_starts=1000,\n",
    "    v_min=-48, # minimum reward\n",
    "    v_max=48, # maximum reward\n",
    "    # n_step=1,\n",
    "    double_q=False,\n",
    "    # double_q=tune.grid_search([True, False]),\n",
    "    num_atoms=1,\n",
    "    noisy=False,\n",
    "    # noisy=tune.grid_search([True, False]),\n",
    "    dueling=False,\n",
    "    # dueling=tune.grid_search([True, False]),\n",
    "    train_batch_size=1200,\n",
    ")\n",
    "config = config.evaluation(\n",
    "    evaluation_interval=1,\n",
    "    evaluation_num_env_runners=4,\n",
    "    # evaluation_parallel_to_training=True,\n",
    "    evaluation_duration=30,\n",
    "    evaluation_config={\n",
    "        \"env_config\": eval_env_config,\n",
    "        \"metrics_num_episodes_for_smoothing\": 4,\n",
    "        # Set explore True for policy gradient algorithms\n",
    "        \"explore\": False,\n",
    "    },\n",
    ")\n",
    "# These settings allows runs to continue after a worker fails for whatever reason.\n",
    "config = config.fault_tolerance(recreate_failed_env_runners=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set stopping criteria for the trials\n",
    "from ray.tune.stopper import CombinedStopper, MaximumIterationStopper, TrialPlateauStopper\n",
    "\n",
    "stopper = CombinedStopper(\n",
    "    MaximumIterationStopper(max_iter=120),\n",
    "    TrialPlateauStopper(metric=\"evaluation/env_runners/episode_reward_mean\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto hyperparameter tuning\n",
    "\n",
    "# Currently, using multiple environments only works when part of the same trial,\n",
    "# because the usernames created from the unique worker number and env number taken from the EnvContext passed in when\n",
    "# initializing a SimpleRLPlayer environment are the same across different trials, resulting in duplicate usernames across trials.\n",
    "\n",
    "analysis = tune.Tuner(\n",
    "    config.algo_class,\n",
    "    param_space=config,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        num_samples=1,\n",
    "        max_concurrent_trials=1,\n",
    "        # reuse_actors=True, # Needs reset_config() to be defined and return True for this algorithm\n",
    "        # Need to get an external library for search algorithms from https://docs.ray.io/en/master/tune/key-concepts.html#tune-search-algorithms\n",
    "        # search_alg= NoneProvided # random search, not ideal.\n",
    "        # scheduler= NoneProvided, # When using concurrent trials, this ends or changes poorly performing trials early.\n",
    "    ),\n",
    "    run_config=train.RunConfig(\n",
    "        name=\"DQN_SimpleRL_v_MaxBP_1\",\n",
    "        storage_path=os.path.abspath(os.path.join(os.getcwd(), os.pardir, 'results')),\n",
    "        stop=stopper,\n",
    "        checkpoint_config=train.CheckpointConfig(\n",
    "            checkpoint_frequency=1,\n",
    "            # checkpoint_score_attribute is the metric to use to determine which checkpoints to keep.\n",
    "            checkpoint_score_attribute=\"evaluation/env_runners/episode_reward_mean\",\n",
    "            # Only the best num_to_keep checkpoints are saved, using checkpoint_score_attribute as the metric to compare.\n",
    "            num_to_keep=1,\n",
    "            # checkpoint_score_order determines whether a higher (\"max\") or lower (\"min\") checkpoint_score_attribute is better.\n",
    "            checkpoint_score_order=\"max\",\n",
    "            # checkpoint_at_end=True\n",
    "        ),\n",
    "    ),\n",
    "\n",
    ").fit()\n",
    "\n",
    "# Wait for training battles to finish closing before continuing to testing.\n",
    "# Without this, showdown may give a nametaken error because the players try to use the same names as in training.\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the path of the best saved checkpoint out of all the trials and iterations in this run.\n",
    "best_result = analysis.get_best_result(\n",
    "    metric=\"evaluation/env_runners/episode_reward_mean\", \n",
    "    mode=\"max\"\n",
    "    )\n",
    "print(best_result)\n",
    "test_checkpoint = best_result.checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Manually load checkpoint from path\n",
    "# If manually loading a checkpoint from a path, you can skip all above cells after SimpleRLPlayer class creation.\n",
    "# The test_checkpoint path should end with the checkpoint_XXXXXX directory, where X's are the checkpoint number with leading 0s.\n",
    "\n",
    "# test_checkpoint = \"../results/DQN_SimpleRL_v_MaxBP_1/DQN_SimpleRLPlayer_9fc91_00010_10_train_batch_size=900_2024-07-26_13-50-33/checkpoint_000019\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Restore the model checkpoint in test_checkpoint\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "\n",
    "test_alg = Algorithm.from_checkpoint(test_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test algorithm against baseline players\n",
    "\n",
    "# Against random player\n",
    "test_env_config = {\n",
    "    'username': 'te_SimpleRL',\n",
    "    'player_config': {\n",
    "        'battle_format': \"gen9randombattle\",\n",
    "        'start_challenging': True,\n",
    "    },\n",
    "    'opponent_class': RandomPlayer,\n",
    "    'opponent_username': 'RandomPlayer',\n",
    "    'opponent_config': {\n",
    "        'battle_format': \"gen9randombattle\",\n",
    "    },\n",
    "}\n",
    "\n",
    "n_battles = 100\n",
    "\n",
    "# Create test environment.\n",
    "test_env = SimpleRLPlayer(env_config=test_env_config)\n",
    "\n",
    "for i in tqdm(range(n_battles), leave=False):\n",
    "    episode_reward = 0\n",
    "    terminated = truncated = False\n",
    "    obs, info = test_env.reset()\n",
    "    while not terminated and not truncated and not (test_env.current_battle is None) and not test_env.current_battle.finished:\n",
    "        action = test_alg.compute_single_action(obs)\n",
    "        obs, reward, terminated, truncated, info = test_env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "print(\"\\n\\nResults against random player:\")\n",
    "print(\n",
    "    f\"DQN Test: {test_env.n_won_battles} victories out of {test_env.n_finished_battles} episodes\"\n",
    ")\n",
    "test_env.close()\n",
    "\n",
    "# Against max base power player\n",
    "test_env_config = {\n",
    "    'username': 'te_SimpleRL',\n",
    "    'player_config': {\n",
    "        'battle_format': \"gen9randombattle\",\n",
    "        'start_challenging': True,\n",
    "    },\n",
    "    'opponent_class': MaxBasePowerPlayer,\n",
    "    'opponent_username': 'MaxBasePower',\n",
    "    'opponent_config': {\n",
    "        'battle_format': \"gen9randombattle\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Create testing environment.\n",
    "test_env = SimpleRLPlayer(env_config=test_env_config)\n",
    "\n",
    "for i in tqdm(range(n_battles), leave=False):\n",
    "    episode_reward = 0\n",
    "    terminated = truncated = False\n",
    "    obs, info = test_env.reset()\n",
    "    while not terminated and not truncated and not (test_env.current_battle is None) and not test_env.current_battle.finished:\n",
    "        action = test_alg.compute_single_action(obs)\n",
    "        obs, reward, terminated, truncated, info = test_env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "print(\"\\nResults against Max Base Power player:\")\n",
    "print(\n",
    "    f\"DQN Test: {test_env.n_won_battles} victories out of {test_env.n_finished_battles} episodes\"\n",
    ")\n",
    "test_env.close()\n",
    "\n",
    "# Against SimpleHeuristics player\n",
    "test_env_config = {\n",
    "    'username': 'te_SimpleRL',\n",
    "    'player_config': {\n",
    "        'battle_format': \"gen9randombattle\",\n",
    "        'start_challenging': True,\n",
    "    },\n",
    "    'opponent_class': SimpleHeuristicsPlayer,\n",
    "    'opponent_username': 'SimpleHeur',\n",
    "    'opponent_config': {\n",
    "        'battle_format': \"gen9randombattle\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Create testing environment.\n",
    "test_env = SimpleRLPlayer(env_config=test_env_config)\n",
    "\n",
    "for i in tqdm(range(n_battles), leave=False):\n",
    "    episode_reward = 0\n",
    "    terminated = truncated = False\n",
    "    obs, info = test_env.reset()\n",
    "    while not terminated and not truncated and not (test_env.current_battle is None) and not test_env.current_battle.finished:\n",
    "        action = test_alg.compute_single_action(obs)\n",
    "        obs, reward, terminated, truncated, info = test_env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "print(\"\\nResults against Simple Heuristic player:\")\n",
    "print(\n",
    "    f\"DQN Test: {test_env.n_won_battles} victories out of {test_env.n_finished_battles} episodes\"\n",
    ")\n",
    "test_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Close the test algorithm to free up resources.\n",
    "test_alg.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
