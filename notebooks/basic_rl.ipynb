{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Local Pokemon Showdown Server\n",
    "cd into your pokemon-showdown directory  \n",
    "node pokemon-showdown start --no-security"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time\n",
    "from typing import List, Any, Union\n",
    "\n",
    "from gymnasium.spaces import Box, Space\n",
    "\n",
    "# from tabulate import tabulate\n",
    "\n",
    "from poke_env.environment.abstract_battle import AbstractBattle\n",
    "from poke_env.data import GenData\n",
    "type_chart = GenData(9).type_chart\n",
    "from poke_env.ps_client.account_configuration import AccountConfiguration, CONFIGURATION_FROM_PLAYER_COUNTER\n",
    "from poke_env.player import (\n",
    "    Gen9EnvSinglePlayer,\n",
    "    MaxBasePowerPlayer,\n",
    "    ObsType,\n",
    "    RandomPlayer,\n",
    "    SimpleHeuristicsPlayer\n",
    ")\n",
    "\n",
    "from ray.rllib.env.env_context import EnvContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def create_account_configuration(username: str, unique_ids: Union[List[Any], None] = None) -> AccountConfiguration:\n",
    "    # If no unique ids provided, create a unique id using the counter.\n",
    "    if unique_ids is None:\n",
    "        # NOTE: This is not thread-safe! Provide unique ids for multithreading / multi workers.\n",
    "        # The 's' in the resulting username indicates single thread.\n",
    "        CONFIGURATION_FROM_PLAYER_COUNTER.update([username])\n",
    "        unique_username = \"%s s %d\" % (username, CONFIGURATION_FROM_PLAYER_COUNTER[username])\n",
    "        if len(unique_username) > 18:\n",
    "            unique_username = \"%s s%d\" % (\n",
    "                username[: 18 - len(unique_username)],\n",
    "                CONFIGURATION_FROM_PLAYER_COUNTER[username],\n",
    "            )\n",
    "    else:\n",
    "        unique_ids_str = ' '.join(map(str, unique_ids))\n",
    "        unique_username = \"%s %s\" % (username, unique_ids_str)\n",
    "        unique_username = unique_username.strip()\n",
    "        if len(unique_username) > 18:\n",
    "            unique_username = \"%s %s\" % (\n",
    "                username[: 18 - len(unique_username)],\n",
    "                unique_ids_str,\n",
    "            )\n",
    "        \n",
    "    return AccountConfiguration(unique_username.strip(), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRLPlayer(Gen9EnvSinglePlayer):\n",
    "    def __init__(self, env_config: Union[EnvContext, dict]):\n",
    "        # Create list of the unique ids of the worker running this env.\n",
    "        if type(env_config) is EnvContext:\n",
    "            # The unique ids from EnvContext are the worker index, and the vector index.\n",
    "            # Worker index is index of the rollout worker that this env is running on, when there are multiple workers.\n",
    "            # Vector index is index of this env on this worker, when there are multiple envs per worker.\n",
    "            unique_ids = [env_config.worker_index, env_config.vector_index]\n",
    "        else:\n",
    "            # Set it to None. This activates create_account_configuration's player counter.\n",
    "            unique_ids = None\n",
    "\n",
    "        # Create unique account configuration for this from username to prevent multithreading naming conflicts\n",
    "        # account_configuration should be None, unless this is created as opponent for self-play\n",
    "        account_configuration = env_config.get('account_configuration')\n",
    "        if account_configuration is None:\n",
    "            # If no username provided, make one from this class name. \n",
    "            username =  env_config.get('username')\n",
    "            if username is None:\n",
    "                username = type(self).__name__\n",
    "            # Create unique account configuration from username, worker process id, and worker env id \n",
    "            account_configuration = create_account_configuration(username=username, unique_ids=unique_ids)\n",
    "        \n",
    "        # if opponent class provided, instantiate it with its config.\n",
    "        opponent_class = env_config.get('opponent_class')\n",
    "        if opponent_class is not None:\n",
    "            # Create unique opponent account configuration if none provided.\n",
    "            # opponent account_configuration should be None when using parallelization or multiple workers\n",
    "            opponent_account_configuration = env_config.get('opponent_account_configuration')\n",
    "            if opponent_account_configuration is None:\n",
    "                # If no opponent username provided, make one from its class name. \n",
    "                opponent_username =  env_config.get('opponent_username')\n",
    "                if opponent_username is None:\n",
    "                    opponent_username = type(opponent_class).__name__\n",
    "                # Create unique opponent account configuration from username, worker process index, and env index on this worker.\n",
    "                opponent_account_configuration = create_account_configuration(username=opponent_username, unique_ids=unique_ids)\n",
    "\n",
    "            # If opponent config provided, use it when instantiating opponent.\n",
    "            opponent_config = env_config.get('opponent_config')\n",
    "            if opponent_config is not None:\n",
    "                # Instantiate the opponent class with opponent config.\n",
    "                opponent = opponent_class(\n",
    "                    account_configuration = opponent_account_configuration,\n",
    "                    **opponent_config\n",
    "                )\n",
    "            else:\n",
    "                # If no opponent config provided, set battle format to be same as SimpleRLPlayer.\n",
    "                opponent = opponent_class(\n",
    "                    account_configuration = opponent_account_configuration, \n",
    "                    battle_format=env_config.get('battle_format')\n",
    "                )\n",
    "        else:\n",
    "            opponent = None\n",
    "        # TODO: Figure out how to only pass arguments if they are not None\n",
    "        super().__init__(\n",
    "            opponent = opponent,\n",
    "            account_configuration = account_configuration,\n",
    "            # avatar = env_config.get('avatar'),\n",
    "            # battle_format = env_config.get('battle_format'),\n",
    "            # log_level = env_config.get('log_level'),\n",
    "            # save_replays = env_config.get('save_replays'),\n",
    "            # server_configuration = env_config.get('server_configuration'),\n",
    "            # start_listening = env_config.get('start_listening'),\n",
    "            # start_timer_on_battle_start = env_config.get('start_timer_on_battle_start'),\n",
    "            # ping_interval = env_config.get('ping_interval'),\n",
    "            # ping_timeout = env_config.get('ping_timeout'),\n",
    "            # team = env_config.get('team'),\n",
    "            # start_challenging = env_config.get('start_challenging')\n",
    "        )\n",
    "        # Try adding this somewhere in init.\n",
    "        # self.reset_battles()\n",
    "\n",
    "\n",
    "    def calc_reward(self, last_battle, current_battle) -> float:\n",
    "        return self.reward_computing_helper(\n",
    "            current_battle, fainted_value=2.0, hp_value=1.0, victory_value=30.0\n",
    "        )\n",
    "\n",
    "    def embed_battle(self, battle: AbstractBattle) -> ObsType:\n",
    "        # -1 indicates that the move does not have a base power\n",
    "        # or is not available\n",
    "        moves_base_power = -np.ones(4)\n",
    "        moves_dmg_multiplier = np.ones(4)\n",
    "        for i, move in enumerate(battle.available_moves):\n",
    "            moves_base_power[i] = (\n",
    "                move.base_power / 100\n",
    "            )  # Simple rescaling to facilitate learning\n",
    "            if move.type:\n",
    "                moves_dmg_multiplier[i] = move.type.damage_multiplier(\n",
    "                    battle.opponent_active_pokemon.type_1,\n",
    "                    battle.opponent_active_pokemon.type_2,\n",
    "                    type_chart=type_chart\n",
    "                )\n",
    "\n",
    "        # We count how many pokemons have fainted in each team\n",
    "        fainted_mon_team = len([mon for mon in battle.team.values() if mon.fainted]) / 6\n",
    "        fainted_mon_opponent = (\n",
    "            len([mon for mon in battle.opponent_team.values() if mon.fainted]) / 6\n",
    "        )\n",
    "\n",
    "        # Final vector with 10 components\n",
    "        final_vector = np.concatenate(\n",
    "            [\n",
    "                moves_base_power,\n",
    "                moves_dmg_multiplier,\n",
    "                [fainted_mon_team, fainted_mon_opponent],\n",
    "            ]\n",
    "        )\n",
    "        return np.float32(final_vector)\n",
    "\n",
    "    def describe_embedding(self) -> Space:\n",
    "        low = [-1, -1, -1, -1, 0, 0, 0, 0, 0, 0]\n",
    "        high = [3, 3, 3, 3, 4, 4, 4, 4, 1, 1]\n",
    "        return Box(\n",
    "            np.array(low, dtype=np.float32),\n",
    "            np.array(high, dtype=np.float32),\n",
    "            dtype=np.float32,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create config\n",
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "from ray import tune, train\n",
    "import os\n",
    "\n",
    "# This is passed to each environment (SimpleRLPlayer) during training\n",
    "train_env_config = {\n",
    "    'username': 'tr_SimpleRL',\n",
    "    'battle_format': \"gen9randombattle\",\n",
    "    'start_challenging': True,\n",
    "    'opponent_class': MaxBasePowerPlayer,\n",
    "    'opponent_username': 'tr_MaxBasePower',\n",
    "    'opponent_config': {\n",
    "        'battle_format': \"gen9randombattle\",\n",
    "    },\n",
    "}\n",
    "# This is passed to each environment (SimpleRLPlayer) during evaluation\n",
    "eval_env_config = {\n",
    "    'username': 'ev_SimpleRL',\n",
    "    'battle_format': \"gen9randombattle\",\n",
    "    'start_challenging': True,\n",
    "    'opponent_class': MaxBasePowerPlayer,\n",
    "    'opponent_username': 'ev_MaxBasePower',\n",
    "    'opponent_config': {\n",
    "        'battle_format': \"gen9randombattle\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Guide to RLLib parameters: https://docs.ray.io/en/latest/rllib/rllib-training.html#common-parameters \n",
    "config = DQNConfig()\n",
    "config = config.environment(env = SimpleRLPlayer, env_config = train_env_config)\n",
    "# Set the framework to use. \"tf2\" for tensorflow, \"torch\" for PyTorch. Devcontainer is set up for Tensorflow.\n",
    "config = config.framework(framework=\"tf2\")\n",
    "config = config.resources(\n",
    "    num_gpus=1,\n",
    "    # num_learner_workers=4,\n",
    "    # num_cpus_per_learner_worker=2,\n",
    "    num_cpus_per_worker=1,\n",
    "    num_cpus_for_local_worker=2\n",
    ")\n",
    "config = config.rollouts(\n",
    "    # Number of workers to run environments. 0 forces rollouts onto the local worker.\n",
    "    num_rollout_workers=20,\n",
    "    num_envs_per_worker=1,\n",
    "    # Don't cut off episodes before they finish when batching.\n",
    "    # As a result, the batch size hyperparameter acts as a minimum and batches may vary in size.\n",
    "    batch_mode=\"complete_episodes\",\n",
    "    # Validation creates environments and does not close them, causes problems.\n",
    "    # validate_env_runners_after_construction=False,\n",
    "    # rollout_fragment_length=\"auto\",\n",
    ")\n",
    "# Set training hyperparameters. \n",
    "# For descriptions, see: https://docs.ray.io/en/latest/rllib/rllib-algorithms.html#deep-q-networks-dqn-rainbow-parametric-dqn\n",
    "config = config.training(\n",
    "    gamma=0.7,\n",
    "    lr=tune.grid_search([1e-3, 5e-4, 1e-4, 5e-5, 1e-5]),\n",
    "    replay_buffer_config={\n",
    "        \"type\": \"MultiAgentPrioritizedReplayBuffer\",\n",
    "        \"capacity\": 100000,\n",
    "    },\n",
    "    num_steps_sampled_before_learning_starts=1000,\n",
    "    # v_min=-48, # minimum reward\n",
    "    # v_max=48, # maximum reward\n",
    "    # n_step=1,\n",
    "    double_q=False,\n",
    "    # double_q=tune.grid_search([True, False]),\n",
    "    num_atoms=1,\n",
    "    noisy=False,\n",
    "    # noisy=tune.grid_search([True, False]),\n",
    "    dueling=False,\n",
    "    # dueling=tune.grid_search([True, False]),\n",
    "    train_batch_size=256\n",
    ")\n",
    "config = config.evaluation(\n",
    "    evaluation_interval=1,\n",
    "    evaluation_num_workers=2,\n",
    "    evaluation_parallel_to_training=True,\n",
    "    evaluation_duration=\"auto\",\n",
    "    evaluation_config={\n",
    "        \"env_config\": eval_env_config,\n",
    "        \"metrics_num_episodes_for_smoothing\": 4,\n",
    "        # Set explore True for policy gradient algorithms\n",
    "        \"explore\": False,\n",
    "    },\n",
    ")\n",
    "config = config.exploration(\n",
    "    explore=True,\n",
    "    exploration_config = {\n",
    "            \"type\": \"EpsilonGreedy\",\n",
    "            \"initial_epsilon\": 1.0,\n",
    "            \"final_epsilon\": 0.05,\n",
    "            \"epsilon_timesteps\": 50000,\n",
    "    }\n",
    ")\n",
    "# These settings allows runs to continue after a worker fails for whatever reason.\n",
    "# config = config.fault_tolerance(recreate_failed_env_runners=True)\n",
    "\n",
    "stop = {\n",
    "    \"episode_reward_mean\": 30,\n",
    "    \"training_iteration\": 120,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-07-16 08:44:29</td></tr>\n",
       "<tr><td>Running for: </td><td>00:03:46.36        </td></tr>\n",
       "<tr><td>Memory:      </td><td>4.8/31.2 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 24.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 2<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                                                    </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_SimpleRLPlayer_16c9f_00000</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-07-16_08-40-40_131016_7391/artifacts/2024-07-16_08-40-43/DQN_SimpleRL_vs_MaxBP/driver_artifacts/DQN_SimpleRLPlayer_16c9f_00000_0_lr=0.0010_2024-07-16_08-40-43/error.txt</td></tr>\n",
       "<tr><td>DQN_SimpleRLPlayer_16c9f_00001</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-07-16_08-40-40_131016_7391/artifacts/2024-07-16_08-40-43/DQN_SimpleRL_vs_MaxBP/driver_artifacts/DQN_SimpleRLPlayer_16c9f_00001_1_lr=0.0005_2024-07-16_08-42-33/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">    lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_SimpleRLPlayer_16c9f_00002</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.0001</td></tr>\n",
       "<tr><td>DQN_SimpleRLPlayer_16c9f_00000</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">0.001 </td></tr>\n",
       "<tr><td>DQN_SimpleRLPlayer_16c9f_00001</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">0.0005</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(DQN pid=9314)\u001b[0m 2024-07-16 08:40:46,105\tWARNING deprecation.py:50 -- DeprecationWarning: `rllib/algorithms/simple_q/` has been deprecated. Use `rllib_contrib/simple_q/` instead. This will raise an error in the future!\n",
      "\u001b[36m(RolloutWorker pid=9530)\u001b[0m /root/.local/lib/python3.8/site-packages/gymnasium/wrappers/compatibility.py:67: DeprecationWarning: \u001b[33mWARN: The `gymnasium.make(..., apply_api_compatibility=...)` parameter is deprecated and will be removed in v0.29. Instead use `gym.make('GymV21Environment-v0', env_name=...)` or `from shimmy import GymV21CompatibilityV0`\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=9530)\u001b[0m   logger.deprecation(\n",
      "\u001b[36m(RolloutWorker pid=9521)\u001b[0m 2024-07-16 08:41:42,718\tWARNING __init__.py:144 -- `config.auto_wrap_old_gym_envs` is activated AND you seem to have provided an old gym-API environment. RLlib will therefore try to auto-fix the following error. However, please consider switching over to the new `gymnasium` APIs:\n",
      "\u001b[36m(RolloutWorker pid=9521)\u001b[0m Your environment ({}) does not abide to the new gymnasium-style API!\n",
      "\u001b[36m(RolloutWorker pid=9521)\u001b[0m From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\n",
      "\u001b[36m(RolloutWorker pid=9521)\u001b[0m {}\n",
      "\u001b[36m(RolloutWorker pid=9521)\u001b[0m Learn more about the most important changes here:\n",
      "\u001b[36m(RolloutWorker pid=9521)\u001b[0m https://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n",
      "\u001b[36m(RolloutWorker pid=9521)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=9521)\u001b[0m In order to fix this problem, do the following:\n",
      "\u001b[36m(RolloutWorker pid=9521)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=9521)\u001b[0m 1) Run `pip install gymnasium` on your command line.\n",
      "\u001b[36m(RolloutWorker pid=9521)\u001b[0m 2) Change all your import statements in your code from\n",
      "\u001b[36m(RolloutWorker pid=9521)\u001b[0m    `import gym` -> `import gymnasium as gym` OR\n",
      "\u001b[36m(RolloutWorker pid=9521)\u001b[0m    `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`\n",
      "\u001b[36m(RolloutWorker pid=9521)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=9521)\u001b[0m For your custom (single agent) gym.Env classes:\n",
      "\u001b[36m(RolloutWorker pid=9521)\u001b[0m 3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n",
      "\u001b[36m(RolloutWorker pid=9521)\u001b[0m      EnvCompatibility` wrapper class.\n",
      "\u001b[36m(RolloutWorker pid=9521)\u001b[0m 3.2) Alternatively to 3.1:\n",
      "\u001b[36m(RolloutWorker pid=9521)\u001b[0m  - Change your `reset()` method to have the call signature 'def reset(self, *,\n",
      "\u001b[36m(RolloutWorker pid=9521)\u001b[0m    seed=None, options=None)'\n",
      "\u001b[36m(RolloutWorker pid=9521)\u001b[0m  - Return an additional info dict (empty dict should be fine) from your `reset()`\n",
      "\u001b[36m(RolloutWorker pid=9521)\u001b[0m    method.\n",
      "\u001b[36m(RolloutWorker pid=9521)\u001b[0m  - Return an additional `truncated` flag from your `step()` method (between `done` and\n",
      "\u001b[36m(RolloutWorker pid=9521)\u001b[0m    `info`). This flag should indicate, whether the episode was terminated prematurely\n",
      "\u001b[36m(RolloutWorker pid=9521)\u001b[0m    due to some time constraint or other kind of horizon setting.\n",
      "\u001b[36m(RolloutWorker pid=9521)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=9521)\u001b[0m For your custom RLlib `MultiAgentEnv` classes:\n",
      "\u001b[36m(RolloutWorker pid=9521)\u001b[0m 4.1) Either wrap your old MultiAgentEnv via the provided\n",
      "\u001b[36m(RolloutWorker pid=9521)\u001b[0m      `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n",
      "\u001b[36m(RolloutWorker pid=9521)\u001b[0m      MultiAgentEnvCompatibility` wrapper class.\n",
      "\u001b[36m(RolloutWorker pid=9521)\u001b[0m 4.2) Alternatively to 4.1:\n",
      "\u001b[36m(RolloutWorker pid=9521)\u001b[0m  - Change your `reset()` method to have the call signature\n",
      "\u001b[36m(RolloutWorker pid=9521)\u001b[0m    'def reset(self, *, seed=None, options=None)'\n",
      "\u001b[36m(RolloutWorker pid=9521)\u001b[0m  - Return an additional per-agent info dict (empty dict should be fine) from your\n",
      "\u001b[36m(RolloutWorker pid=9521)\u001b[0m    `reset()` method.\n",
      "\u001b[36m(RolloutWorker pid=9521)\u001b[0m  - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n",
      "\u001b[36m(RolloutWorker pid=9521)\u001b[0m    done (as opposed to has been terminated prematurely due to some horizon/time-limit\n",
      "\u001b[36m(RolloutWorker pid=9521)\u001b[0m    setting).\n",
      "\u001b[36m(RolloutWorker pid=9521)\u001b[0m  - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n",
      "\u001b[36m(RolloutWorker pid=9521)\u001b[0m    method, including the `__all__` key (100% analogous to your `dones/terminateds`\n",
      "\u001b[36m(RolloutWorker pid=9521)\u001b[0m    per-agent dict).\n",
      "\u001b[36m(RolloutWorker pid=9521)\u001b[0m    Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n",
      "\u001b[36m(RolloutWorker pid=9521)\u001b[0m    flag should indicate, whether the episode (for some agent or all agents) was\n",
      "\u001b[36m(RolloutWorker pid=9521)\u001b[0m    terminated prematurely due to some time constraint or other kind of horizon setting.\n",
      "\u001b[36m(RolloutWorker pid=9521)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=9530)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=9530, ip=172.17.0.2, actor_id=0260c03725f3f5ca2eee6be001000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f3f133c1490>)\n",
      "\u001b[36m(RolloutWorker pid=9530)\u001b[0m   File \"/root/.local/lib/python3.8/site-packages/poke_env/player/openai_api.py\", line 319, in reset\n",
      "\u001b[36m(RolloutWorker pid=9530)\u001b[0m     raise RuntimeError(\"Agent is not challenging\")\n",
      "\u001b[36m(RolloutWorker pid=9530)\u001b[0m RuntimeError: Agent is not challenging\n",
      "\u001b[36m(RolloutWorker pid=9530)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(RolloutWorker pid=9530)\u001b[0m \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=9530, ip=172.17.0.2, actor_id=0260c03725f3f5ca2eee6be001000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f3f133c1490>)\n",
      "\u001b[36m(RolloutWorker pid=9530)\u001b[0m   File \"/root/.local/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 407, in __init__\n",
      "\u001b[36m(RolloutWorker pid=9530)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[36m(RolloutWorker pid=9530)\u001b[0m   File \"/root/.local/lib/python3.8/site-packages/ray/rllib/env/utils/__init__.py\", line 158, in _gym_env_creator\n",
      "\u001b[36m(RolloutWorker pid=9530)\u001b[0m     obs_and_infos = env.reset(seed=None, options={})\n",
      "\u001b[36m(RolloutWorker pid=9530)\u001b[0m   File \"/root/.local/lib/python3.8/site-packages/gymnasium/wrappers/compatibility.py\", line 100, in reset\n",
      "\u001b[36m(RolloutWorker pid=9530)\u001b[0m     return self.env.reset(), {}\n",
      "\u001b[36m(RolloutWorker pid=9530)\u001b[0m   File \"/root/.local/lib/python3.8/site-packages/poke_env/player/openai_api.py\", line 319, in reset\n",
      "\u001b[36m(RolloutWorker pid=9530)\u001b[0m     raise RuntimeError(\"Agent is not challenging\")\n",
      "\u001b[36m(RolloutWorker pid=9530)\u001b[0m RuntimeError: Agent is not challenging\n",
      "\u001b[36m(RolloutWorker pid=9550)\u001b[0m /root/.local/lib/python3.8/site-packages/gymnasium/wrappers/compatibility.py:67: DeprecationWarning: \u001b[33mWARN: The `gymnasium.make(..., apply_api_compatibility=...)` parameter is deprecated and will be removed in v0.29. Instead use `gym.make('GymV21Environment-v0', env_name=...)` or `from shimmy import GymV21CompatibilityV0`\u001b[0m\u001b[32m [repeated 19x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=9550)\u001b[0m   logger.deprecation(\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=9530)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m 2024-07-16 08:42:33,043\tERROR actor_manager.py:517 -- Ray error, taking actor 1 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=9521, ip=172.17.0.2, actor_id=dcfcb4b61fb5ae3c78d16e0301000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f0712317490>)\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m 2024-07-16 08:42:33,044\tERROR actor_manager.py:517 -- Ray error, taking actor 2 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=9522, ip=172.17.0.2, actor_id=4719fda7d85403246eaec2c201000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f499ccd63d0>)\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m 2024-07-16 08:42:33,044\tERROR actor_manager.py:517 -- Ray error, taking actor 3 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=9523, ip=172.17.0.2, actor_id=b677123e080073a1f9f450fb01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f0dc4bed460>)\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m 2024-07-16 08:42:33,045\tERROR actor_manager.py:517 -- Ray error, taking actor 4 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=9524, ip=172.17.0.2, actor_id=61553145c742acfe69de2a6f01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f32e439b340>)\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m 2024-07-16 08:42:33,045\tERROR actor_manager.py:517 -- Ray error, taking actor 5 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=9525, ip=172.17.0.2, actor_id=036df3df0bea492d96084df701000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fd0bc522310>)\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m 2024-07-16 08:42:33,045\tERROR actor_manager.py:517 -- Ray error, taking actor 6 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=9526, ip=172.17.0.2, actor_id=83e5240663a2e5909419bdba01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fc028bb9460>)\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m 2024-07-16 08:42:33,046\tERROR actor_manager.py:517 -- Ray error, taking actor 7 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=9528, ip=172.17.0.2, actor_id=1be76eaa65366742fa157dcf01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f588769c490>)\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m 2024-07-16 08:42:33,046\tERROR actor_manager.py:517 -- Ray error, taking actor 8 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=9530, ip=172.17.0.2, actor_id=0260c03725f3f5ca2eee6be001000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f3f133c1490>)\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m 2024-07-16 08:42:33,046\tERROR actor_manager.py:517 -- Ray error, taking actor 9 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=9532, ip=172.17.0.2, actor_id=dad53a7f25d4c358100c793201000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f1da5ded310>)\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m 2024-07-16 08:42:33,046\tERROR actor_manager.py:517 -- Ray error, taking actor 10 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=9535, ip=172.17.0.2, actor_id=89e07f53fa0458f3c26d810301000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7efab19d5460>)\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m 2024-07-16 08:42:33,046\tERROR actor_manager.py:517 -- Ray error, taking actor 11 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=9536, ip=172.17.0.2, actor_id=1f92d966df15fc1cc24443a701000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f45f6ce0400>)\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m 2024-07-16 08:42:33,047\tERROR actor_manager.py:517 -- Ray error, taking actor 12 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=9539, ip=172.17.0.2, actor_id=4c63573afe2c6cdf8a9a706801000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f0dca0e14c0>)\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m 2024-07-16 08:42:33,047\tERROR actor_manager.py:517 -- Ray error, taking actor 13 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=9540, ip=172.17.0.2, actor_id=0c175a80255a4bb36beb7a0f01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fc5856bd340>)\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m 2024-07-16 08:42:33,047\tERROR actor_manager.py:517 -- Ray error, taking actor 14 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=9543, ip=172.17.0.2, actor_id=c78239788d40a624484d500d01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f69376a5460>)\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m 2024-07-16 08:42:33,048\tERROR actor_manager.py:517 -- Ray error, taking actor 15 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=9546, ip=172.17.0.2, actor_id=28424f4d010b640215e01eb501000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f710f4b21f0>)\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m 2024-07-16 08:42:33,048\tERROR actor_manager.py:517 -- Ray error, taking actor 16 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=9548, ip=172.17.0.2, actor_id=2bbb1f73ee27fdc78cb8fd3e01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f4eb99e5400>)\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m 2024-07-16 08:42:33,048\tERROR actor_manager.py:517 -- Ray error, taking actor 17 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=9550, ip=172.17.0.2, actor_id=fbeaf8680c44e168b7c6994201000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f1d7c404400>)\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m 2024-07-16 08:42:33,048\tERROR actor_manager.py:517 -- Ray error, taking actor 18 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=9554, ip=172.17.0.2, actor_id=cf0102ec0dad9f178484c4a201000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f50bcd642e0>)\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m 2024-07-16 08:42:33,048\tERROR actor_manager.py:517 -- Ray error, taking actor 19 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=9556, ip=172.17.0.2, actor_id=693e287f9ccc250249ba8d9b01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f6625832460>)\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m 2024-07-16 08:42:33,049\tERROR actor_manager.py:517 -- Ray error, taking actor 20 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=9558, ip=172.17.0.2, actor_id=c6ef84a1ee307e32172ced1401000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f584b0fe460>)\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::DQN.__init__()\u001b[39m (pid=9314, ip=172.17.0.2, actor_id=b74c00c47f010c081e129c1301000000, repr=DQN)\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m   File \"/root/.local/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 229, in _setup\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m     self.add_workers(\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m   File \"/root/.local/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 682, in add_workers\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m     raise result.get()\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m   File \"/root/.local/lib/python3.8/site-packages/ray/rllib/utils/actor_manager.py\", line 497, in _fetch_result\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m     result = ray.get(r)\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=9521, ip=172.17.0.2, actor_id=dcfcb4b61fb5ae3c78d16e0301000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f0712317490>)\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m \u001b[36mray::DQN.__init__()\u001b[39m (pid=9314, ip=172.17.0.2, actor_id=b74c00c47f010c081e129c1301000000, repr=DQN)\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m   File \"/root/.local/lib/python3.8/site-packages/ray/rllib/utils/deprecation.py\", line 109, in patched_init\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m     return obj_init(*args, **kwargs)\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m     super().__init__(\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m   File \"/root/.local/lib/python3.8/site-packages/ray/rllib/algorithms/algorithm.py\", line 631, in setup\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m     raise e.args[0].args[2]\n",
      "2024-07-16 08:42:33,071\tERROR tune_controller.py:1332 -- Trial task failed for trial DQN_SimpleRLPlayer_16c9f_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/.local/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/ray/_private/worker.py\", line 2667, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::DQN.__init__()\u001b[39m (pid=9314, ip=172.17.0.2, actor_id=b74c00c47f010c081e129c1301000000, repr=DQN)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 229, in _setup\n",
      "    self.add_workers(\n",
      "  File \"/root/.local/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 682, in add_workers\n",
      "    raise result.get()\n",
      "  File \"/root/.local/lib/python3.8/site-packages/ray/rllib/utils/actor_manager.py\", line 497, in _fetch_result\n",
      "    result = ray.get(r)\n",
      "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=9521, ip=172.17.0.2, actor_id=dcfcb4b61fb5ae3c78d16e0301000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f0712317490>)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/poke_env/player/openai_api.py\", line 319, in reset\n",
      "    raise RuntimeError(\"Agent is not challenging\")\n",
      "RuntimeError: Agent is not challenging\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=9521, ip=172.17.0.2, actor_id=dcfcb4b61fb5ae3c78d16e0301000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f0712317490>)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 407, in __init__\n",
      "    self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "  File \"/root/.local/lib/python3.8/site-packages/ray/rllib/env/utils/__init__.py\", line 158, in _gym_env_creator\n",
      "    obs_and_infos = env.reset(seed=None, options={})\n",
      "  File \"/root/.local/lib/python3.8/site-packages/gymnasium/wrappers/compatibility.py\", line 100, in reset\n",
      "    return self.env.reset(), {}\n",
      "  File \"/root/.local/lib/python3.8/site-packages/poke_env/player/openai_api.py\", line 319, in reset\n",
      "    raise RuntimeError(\"Agent is not challenging\")\n",
      "RuntimeError: Agent is not challenging\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::DQN.__init__()\u001b[39m (pid=9314, ip=172.17.0.2, actor_id=b74c00c47f010c081e129c1301000000, repr=DQN)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/ray/rllib/utils/deprecation.py\", line 109, in patched_init\n",
      "    return obj_init(*args, **kwargs)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/ray/rllib/algorithms/algorithm.py\", line 533, in __init__\n",
      "    super().__init__(\n",
      "  File \"/root/.local/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\", line 161, in __init__\n",
      "    self.setup(copy.deepcopy(self.config))\n",
      "  File \"/root/.local/lib/python3.8/site-packages/ray/rllib/algorithms/algorithm.py\", line 631, in setup\n",
      "    self.workers = WorkerSet(\n",
      "  File \"/root/.local/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 181, in __init__\n",
      "    raise e.args[0].args[2]\n",
      "RuntimeError: Agent is not challenging\n",
      "\u001b[36m(DQN pid=11074)\u001b[0m 2024-07-16 08:42:37,616\tWARNING deprecation.py:50 -- DeprecationWarning: `rllib/algorithms/simple_q/` has been deprecated. Use `rllib_contrib/simple_q/` instead. This will raise an error in the future!\n",
      "\u001b[36m(RolloutWorker pid=9550)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=9550, ip=172.17.0.2, actor_id=fbeaf8680c44e168b7c6994201000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f1d7c404400>)\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m   File \"/root/.local/lib/python3.8/site-packages/poke_env/player/openai_api.py\", line 319, in reset\u001b[32m [repeated 120x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m     raise RuntimeError(\"Agent is not challenging\")\u001b[32m [repeated 80x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m RuntimeError: Agent is not challenging\u001b[32m [repeated 81x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m During handling of the above exception, another exception occurred:\u001b[32m [repeated 41x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=9521, ip=172.17.0.2, actor_id=dcfcb4b61fb5ae3c78d16e0301000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f0712317490>)\u001b[32m [repeated 40x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m   File \"/root/.local/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 181, in __init__\u001b[32m [repeated 43x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\u001b[32m [repeated 40x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m   File \"/root/.local/lib/python3.8/site-packages/ray/rllib/env/utils/__init__.py\", line 158, in _gym_env_creator\u001b[32m [repeated 40x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m     obs_and_infos = env.reset(seed=None, options={})\u001b[32m [repeated 40x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m     return self.env.reset(), {}\u001b[32m [repeated 40x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=11143)\u001b[0m /root/.local/lib/python3.8/site-packages/gymnasium/wrappers/compatibility.py:67: DeprecationWarning: \u001b[33mWARN: The `gymnasium.make(..., apply_api_compatibility=...)` parameter is deprecated and will be removed in v0.29. Instead use `gym.make('GymV21Environment-v0', env_name=...)` or `from shimmy import GymV21CompatibilityV0`\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=11143)\u001b[0m   logger.deprecation(\n",
      "\u001b[36m(DQN pid=9314)\u001b[0m \u001b[32m [repeated 82x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=11144)\u001b[0m /root/.local/lib/python3.8/site-packages/gymnasium/wrappers/compatibility.py:67: DeprecationWarning: \u001b[33mWARN: The `gymnasium.make(..., apply_api_compatibility=...)` parameter is deprecated and will be removed in v0.29. Instead use `gym.make('GymV21Environment-v0', env_name=...)` or `from shimmy import GymV21CompatibilityV0`\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=11144)\u001b[0m   logger.deprecation(\n",
      "\u001b[36m(RolloutWorker pid=11142)\u001b[0m 2024-07-16 08:43:34,793\tWARNING __init__.py:144 -- `config.auto_wrap_old_gym_envs` is activated AND you seem to have provided an old gym-API environment. RLlib will therefore try to auto-fix the following error. However, please consider switching over to the new `gymnasium` APIs:\n",
      "\u001b[36m(RolloutWorker pid=11142)\u001b[0m Your environment ({}) does not abide to the new gymnasium-style API!\n",
      "\u001b[36m(RolloutWorker pid=11142)\u001b[0m From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\n",
      "\u001b[36m(RolloutWorker pid=11142)\u001b[0m {}\n",
      "\u001b[36m(RolloutWorker pid=11142)\u001b[0m Learn more about the most important changes here:\n",
      "\u001b[36m(RolloutWorker pid=11142)\u001b[0m https://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n",
      "\u001b[36m(RolloutWorker pid=11142)\u001b[0m In order to fix this problem, do the following:\n",
      "\u001b[36m(RolloutWorker pid=11142)\u001b[0m 1) Run `pip install gymnasium` on your command line.\n",
      "\u001b[36m(RolloutWorker pid=11142)\u001b[0m 2) Change all your import statements in your code from\n",
      "\u001b[36m(RolloutWorker pid=11142)\u001b[0m    `import gym` -> `import gymnasium as gym` OR\n",
      "\u001b[36m(RolloutWorker pid=11142)\u001b[0m    `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`\n",
      "\u001b[36m(RolloutWorker pid=11142)\u001b[0m For your custom (single agent) gym.Env classes:\n",
      "\u001b[36m(RolloutWorker pid=11142)\u001b[0m 3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n",
      "\u001b[36m(RolloutWorker pid=11142)\u001b[0m      EnvCompatibility` wrapper class.\n",
      "\u001b[36m(RolloutWorker pid=11142)\u001b[0m 3.2) Alternatively to 3.1:\n",
      "\u001b[36m(RolloutWorker pid=11142)\u001b[0m  - Change your `reset()` method to have the call signature 'def reset(self, *,\n",
      "\u001b[36m(RolloutWorker pid=11142)\u001b[0m    seed=None, options=None)'\n",
      "\u001b[36m(RolloutWorker pid=11142)\u001b[0m  - Return an additional info dict (empty dict should be fine) from your `reset()`\n",
      "\u001b[36m(RolloutWorker pid=11142)\u001b[0m    method.\n",
      "\u001b[36m(RolloutWorker pid=11142)\u001b[0m  - Return an additional `truncated` flag from your `step()` method (between `done` and\n",
      "\u001b[36m(RolloutWorker pid=11142)\u001b[0m    `info`). This flag should indicate, whether the episode was terminated prematurely\n",
      "\u001b[36m(RolloutWorker pid=11142)\u001b[0m    due to some time constraint or other kind of horizon setting.\n",
      "\u001b[36m(RolloutWorker pid=11142)\u001b[0m For your custom RLlib `MultiAgentEnv` classes:\n",
      "\u001b[36m(RolloutWorker pid=11142)\u001b[0m 4.1) Either wrap your old MultiAgentEnv via the provided\n",
      "\u001b[36m(RolloutWorker pid=11142)\u001b[0m      `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n",
      "\u001b[36m(RolloutWorker pid=11142)\u001b[0m      MultiAgentEnvCompatibility` wrapper class.\n",
      "\u001b[36m(RolloutWorker pid=11142)\u001b[0m 4.2) Alternatively to 4.1:\n",
      "\u001b[36m(RolloutWorker pid=11142)\u001b[0m  - Change your `reset()` method to have the call signature\n",
      "\u001b[36m(RolloutWorker pid=11142)\u001b[0m    'def reset(self, *, seed=None, options=None)'\n",
      "\u001b[36m(RolloutWorker pid=11142)\u001b[0m  - Return an additional per-agent info dict (empty dict should be fine) from your\n",
      "\u001b[36m(RolloutWorker pid=11142)\u001b[0m    `reset()` method.\n",
      "\u001b[36m(RolloutWorker pid=11142)\u001b[0m  - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n",
      "\u001b[36m(RolloutWorker pid=11142)\u001b[0m    done (as opposed to has been terminated prematurely due to some horizon/time-limit\n",
      "\u001b[36m(RolloutWorker pid=11142)\u001b[0m    setting).\n",
      "\u001b[36m(RolloutWorker pid=11142)\u001b[0m  - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n",
      "\u001b[36m(RolloutWorker pid=11142)\u001b[0m    method, including the `__all__` key (100% analogous to your `dones/terminateds`\n",
      "\u001b[36m(RolloutWorker pid=11142)\u001b[0m    per-agent dict).\n",
      "\u001b[36m(RolloutWorker pid=11142)\u001b[0m    Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n",
      "\u001b[36m(RolloutWorker pid=11142)\u001b[0m    flag should indicate, whether the episode (for some agent or all agents) was\n",
      "\u001b[36m(RolloutWorker pid=11142)\u001b[0m    terminated prematurely due to some time constraint or other kind of horizon setting.\n",
      "\u001b[36m(RolloutWorker pid=11143)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=11143, ip=172.17.0.2, actor_id=c5258630551df14610a1af3b01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f7520754460>)\n",
      "\u001b[36m(RolloutWorker pid=11143)\u001b[0m   File \"/root/.local/lib/python3.8/site-packages/poke_env/player/openai_api.py\", line 319, in reset\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=11143)\u001b[0m     raise RuntimeError(\"Agent is not challenging\")\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=11143)\u001b[0m RuntimeError: Agent is not challenging\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=11143)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(RolloutWorker pid=11143)\u001b[0m \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=11143, ip=172.17.0.2, actor_id=c5258630551df14610a1af3b01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f7520754460>)\n",
      "\u001b[36m(RolloutWorker pid=11143)\u001b[0m   File \"/root/.local/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 407, in __init__\n",
      "\u001b[36m(RolloutWorker pid=11143)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[36m(RolloutWorker pid=11143)\u001b[0m   File \"/root/.local/lib/python3.8/site-packages/ray/rllib/env/utils/__init__.py\", line 158, in _gym_env_creator\n",
      "\u001b[36m(RolloutWorker pid=11143)\u001b[0m     obs_and_infos = env.reset(seed=None, options={})\n",
      "\u001b[36m(RolloutWorker pid=11143)\u001b[0m     return self.env.reset(), {}\n",
      "\u001b[36m(RolloutWorker pid=11143)\u001b[0m \u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=11175)\u001b[0m /root/.local/lib/python3.8/site-packages/gymnasium/wrappers/compatibility.py:67: DeprecationWarning: \u001b[33mWARN: The `gymnasium.make(..., apply_api_compatibility=...)` parameter is deprecated and will be removed in v0.29. Instead use `gym.make('GymV21Environment-v0', env_name=...)` or `from shimmy import GymV21CompatibilityV0`\u001b[0m\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=11175)\u001b[0m   logger.deprecation(\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=11144)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=11144, ip=172.17.0.2, actor_id=d5f4c27396c7edbb4118c49801000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f328c9403a0>)\n",
      "\u001b[36m(RolloutWorker pid=11144)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(RolloutWorker pid=11144)\u001b[0m \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=11144, ip=172.17.0.2, actor_id=d5f4c27396c7edbb4118c49801000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f328c9403a0>)\n",
      "\u001b[36m(RolloutWorker pid=11144)\u001b[0m   File \"/root/.local/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 407, in __init__\n",
      "\u001b[36m(RolloutWorker pid=11144)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[36m(RolloutWorker pid=11144)\u001b[0m   File \"/root/.local/lib/python3.8/site-packages/ray/rllib/env/utils/__init__.py\", line 158, in _gym_env_creator\n",
      "\u001b[36m(RolloutWorker pid=11144)\u001b[0m     obs_and_infos = env.reset(seed=None, options={})\n",
      "\u001b[36m(RolloutWorker pid=11144)\u001b[0m     return self.env.reset(), {}\n",
      "2024-07-16 08:44:25,161\tERROR tune_controller.py:1332 -- Trial task failed for trial DQN_SimpleRLPlayer_16c9f_00001\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/.local/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/ray/_private/worker.py\", line 2667, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::DQN.__init__()\u001b[39m (pid=11074, ip=172.17.0.2, actor_id=550946fcdb3f8af5cc412f2d01000000, repr=DQN)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 229, in _setup\n",
      "    self.add_workers(\n",
      "  File \"/root/.local/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 682, in add_workers\n",
      "    raise result.get()\n",
      "  File \"/root/.local/lib/python3.8/site-packages/ray/rllib/utils/actor_manager.py\", line 497, in _fetch_result\n",
      "    result = ray.get(r)\n",
      "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=11142, ip=172.17.0.2, actor_id=e5ebd836451a890bb9dece1001000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f764bbd64f0>)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/poke_env/player/openai_api.py\", line 319, in reset\n",
      "    raise RuntimeError(\"Agent is not challenging\")\n",
      "RuntimeError: Agent is not challenging\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=11142, ip=172.17.0.2, actor_id=e5ebd836451a890bb9dece1001000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f764bbd64f0>)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 407, in __init__\n",
      "    self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "  File \"/root/.local/lib/python3.8/site-packages/ray/rllib/env/utils/__init__.py\", line 158, in _gym_env_creator\n",
      "    obs_and_infos = env.reset(seed=None, options={})\n",
      "  File \"/root/.local/lib/python3.8/site-packages/gymnasium/wrappers/compatibility.py\", line 100, in reset\n",
      "    return self.env.reset(), {}\n",
      "  File \"/root/.local/lib/python3.8/site-packages/poke_env/player/openai_api.py\", line 319, in reset\n",
      "    raise RuntimeError(\"Agent is not challenging\")\n",
      "RuntimeError: Agent is not challenging\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::DQN.__init__()\u001b[39m (pid=11074, ip=172.17.0.2, actor_id=550946fcdb3f8af5cc412f2d01000000, repr=DQN)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/ray/rllib/utils/deprecation.py\", line 109, in patched_init\n",
      "    return obj_init(*args, **kwargs)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/ray/rllib/algorithms/algorithm.py\", line 533, in __init__\n",
      "    super().__init__(\n",
      "  File \"/root/.local/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\", line 161, in __init__\n",
      "    self.setup(copy.deepcopy(self.config))\n",
      "  File \"/root/.local/lib/python3.8/site-packages/ray/rllib/algorithms/algorithm.py\", line 631, in setup\n",
      "    self.workers = WorkerSet(\n",
      "  File \"/root/.local/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 181, in __init__\n",
      "    raise e.args[0].args[2]\n",
      "RuntimeError: Agent is not challenging\n",
      "\u001b[36m(DQN pid=11074)\u001b[0m 2024-07-16 08:44:25,134\tERROR actor_manager.py:517 -- Ray error, taking actor 1 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=11142, ip=172.17.0.2, actor_id=e5ebd836451a890bb9dece1001000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f764bbd64f0>)\n",
      "\u001b[36m(DQN pid=11074)\u001b[0m 2024-07-16 08:44:25,134\tERROR actor_manager.py:517 -- Ray error, taking actor 2 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=11143, ip=172.17.0.2, actor_id=c5258630551df14610a1af3b01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f7520754460>)\n",
      "\u001b[36m(DQN pid=11074)\u001b[0m 2024-07-16 08:44:25,134\tERROR actor_manager.py:517 -- Ray error, taking actor 3 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=11144, ip=172.17.0.2, actor_id=d5f4c27396c7edbb4118c49801000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f328c9403a0>)\n",
      "\u001b[36m(DQN pid=11074)\u001b[0m 2024-07-16 08:44:25,135\tERROR actor_manager.py:517 -- Ray error, taking actor 4 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=11145, ip=172.17.0.2, actor_id=53b062bb476a7ae51b1fc04c01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f822e713460>)\n",
      "\u001b[36m(DQN pid=11074)\u001b[0m 2024-07-16 08:44:25,135\tERROR actor_manager.py:517 -- Ray error, taking actor 5 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=11146, ip=172.17.0.2, actor_id=1a67b6608bb995cecf7f84df01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fdc58669460>)\n",
      "\u001b[36m(DQN pid=11074)\u001b[0m 2024-07-16 08:44:25,135\tERROR actor_manager.py:517 -- Ray error, taking actor 6 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=11147, ip=172.17.0.2, actor_id=3a364518324210d64908b04101000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fc87dc5a2b0>)\n",
      "\u001b[36m(DQN pid=11074)\u001b[0m 2024-07-16 08:44:25,135\tERROR actor_manager.py:517 -- Ray error, taking actor 7 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=11149, ip=172.17.0.2, actor_id=76625fd9d88679b04a241ee501000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fd29576d370>)\n",
      "\u001b[36m(DQN pid=11074)\u001b[0m 2024-07-16 08:44:25,135\tERROR actor_manager.py:517 -- Ray error, taking actor 8 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=11152, ip=172.17.0.2, actor_id=701df7f75f2d28ec003e15e901000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fca4149c490>)\n",
      "\u001b[36m(DQN pid=11074)\u001b[0m 2024-07-16 08:44:25,135\tERROR actor_manager.py:517 -- Ray error, taking actor 9 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=11153, ip=172.17.0.2, actor_id=97f61576fa663978668089e701000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fd8e319e490>)\n",
      "\u001b[36m(DQN pid=11074)\u001b[0m 2024-07-16 08:44:25,135\tERROR actor_manager.py:517 -- Ray error, taking actor 10 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=11155, ip=172.17.0.2, actor_id=37492b0b31c5f72b91a030d301000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f148cb87490>)\n",
      "\u001b[36m(DQN pid=11074)\u001b[0m 2024-07-16 08:44:25,135\tERROR actor_manager.py:517 -- Ray error, taking actor 11 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=11157, ip=172.17.0.2, actor_id=1b8352b647f8bab70522bcb201000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fc1af42c400>)\n",
      "\u001b[36m(DQN pid=11074)\u001b[0m 2024-07-16 08:44:25,135\tERROR actor_manager.py:517 -- Ray error, taking actor 12 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=11160, ip=172.17.0.2, actor_id=a31981f27f84d0126c86d40b01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f80d42bc370>)\n",
      "\u001b[36m(DQN pid=11074)\u001b[0m 2024-07-16 08:44:25,136\tERROR actor_manager.py:517 -- Ray error, taking actor 13 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=11162, ip=172.17.0.2, actor_id=27c741562fb51f27e1abf50b01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f0d0691d3d0>)\n",
      "\u001b[36m(DQN pid=11074)\u001b[0m 2024-07-16 08:44:25,136\tERROR actor_manager.py:517 -- Ray error, taking actor 14 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=11165, ip=172.17.0.2, actor_id=e85887cbf3cf170f941d92c801000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fa952146340>)\n",
      "\u001b[36m(DQN pid=11074)\u001b[0m 2024-07-16 08:44:25,136\tERROR actor_manager.py:517 -- Ray error, taking actor 15 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=11167, ip=172.17.0.2, actor_id=cbd5ca0f0540f933e4c273a301000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fbbc964d430>)\n",
      "\u001b[36m(DQN pid=11074)\u001b[0m 2024-07-16 08:44:25,136\tERROR actor_manager.py:517 -- Ray error, taking actor 16 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=11169, ip=172.17.0.2, actor_id=2a90186396c6a0a4968480fb01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f698f0b34f0>)\n",
      "\u001b[36m(DQN pid=11074)\u001b[0m 2024-07-16 08:44:25,136\tERROR actor_manager.py:517 -- Ray error, taking actor 17 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=11171, ip=172.17.0.2, actor_id=973f0e6e511ad0fe2eee31a201000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f5bd820c430>)\n",
      "\u001b[36m(DQN pid=11074)\u001b[0m 2024-07-16 08:44:25,137\tERROR actor_manager.py:517 -- Ray error, taking actor 18 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=11172, ip=172.17.0.2, actor_id=b9f224a8cabf0f0d72d8391f01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f23bdd5b400>)\n",
      "\u001b[36m(DQN pid=11074)\u001b[0m 2024-07-16 08:44:25,137\tERROR actor_manager.py:517 -- Ray error, taking actor 19 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=11175, ip=172.17.0.2, actor_id=5ff30ab83007d6304b5764bd01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fc408a5c310>)\n",
      "\u001b[36m(DQN pid=11074)\u001b[0m 2024-07-16 08:44:25,137\tERROR actor_manager.py:517 -- Ray error, taking actor 20 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=11177, ip=172.17.0.2, actor_id=b2baa20b27183e3e0fb714dc01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fc437f9c4f0>)\n",
      "\u001b[36m(DQN pid=11074)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::DQN.__init__()\u001b[39m (pid=11074, ip=172.17.0.2, actor_id=550946fcdb3f8af5cc412f2d01000000, repr=DQN)\n",
      "\u001b[36m(DQN pid=11074)\u001b[0m   File \"/root/.local/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 229, in _setup\n",
      "\u001b[36m(DQN pid=11074)\u001b[0m     self.add_workers(\n",
      "\u001b[36m(DQN pid=11074)\u001b[0m   File \"/root/.local/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 682, in add_workers\n",
      "\u001b[36m(DQN pid=11074)\u001b[0m     raise result.get()\n",
      "\u001b[36m(DQN pid=11074)\u001b[0m   File \"/root/.local/lib/python3.8/site-packages/ray/rllib/utils/actor_manager.py\", line 497, in _fetch_result\n",
      "\u001b[36m(DQN pid=11074)\u001b[0m     result = ray.get(r)\n",
      "\u001b[36m(DQN pid=11074)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=11142, ip=172.17.0.2, actor_id=e5ebd836451a890bb9dece1001000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f764bbd64f0>)\n",
      "\u001b[36m(DQN pid=11074)\u001b[0m \u001b[36mray::DQN.__init__()\u001b[39m (pid=11074, ip=172.17.0.2, actor_id=550946fcdb3f8af5cc412f2d01000000, repr=DQN)\n",
      "\u001b[36m(DQN pid=11074)\u001b[0m   File \"/root/.local/lib/python3.8/site-packages/ray/rllib/utils/deprecation.py\", line 109, in patched_init\n",
      "\u001b[36m(DQN pid=11074)\u001b[0m     return obj_init(*args, **kwargs)\n",
      "\u001b[36m(DQN pid=11074)\u001b[0m     super().__init__(\n",
      "\u001b[36m(DQN pid=11074)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[36m(DQN pid=11074)\u001b[0m   File \"/root/.local/lib/python3.8/site-packages/ray/rllib/algorithms/algorithm.py\", line 631, in setup\n",
      "\u001b[36m(DQN pid=11074)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[36m(DQN pid=11074)\u001b[0m     raise e.args[0].args[2]\n",
      "\u001b[36m(DQN pid=12689)\u001b[0m 2024-07-16 08:44:29,879\tWARNING deprecation.py:50 -- DeprecationWarning: `rllib/algorithms/simple_q/` has been deprecated. Use `rllib_contrib/simple_q/` instead. This will raise an error in the future!\n",
      "\u001b[36m(RolloutWorker pid=11175)\u001b[0m   File \"/root/.local/lib/python3.8/site-packages/poke_env/player/openai_api.py\", line 319, in reset\u001b[32m [repeated 120x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=11175)\u001b[0m     raise RuntimeError(\"Agent is not challenging\")\u001b[32m [repeated 80x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=11175)\u001b[0m RuntimeError: Agent is not challenging\u001b[32m [repeated 81x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=11175)\u001b[0m \u001b[32m [repeated 82x across cluster]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Auto hyperparameter tuning\n",
    "\n",
    "# Currently, using multiple environments only works when part of the same trial,\n",
    "# because the usernames created from the unique worker number and env number taken from the EnvContext passed in when\n",
    "# initializing a SimpleRLPlayer environment are the same across different trials, resulting in duplicate usernames across trials.\n",
    "\n",
    "analysis = tune.Tuner(\n",
    "    config.algo_class,\n",
    "    param_space=config,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        num_samples=8,\n",
    "        max_concurrent_trials=1,\n",
    "        # reuse_actors=True, # Needs reset_config() to be defined and return True for this algorithm\n",
    "        # Need to get an external library for search algorithms from https://docs.ray.io/en/master/tune/key-concepts.html#tune-search-algorithms\n",
    "        # search_alg= NoneProvided # random search, not ideal.\n",
    "        # scheduler= NoneProvided, # When using concurrent trials, this ends or changes poorly performing trials early.\n",
    "    ),\n",
    "    run_config=train.RunConfig(\n",
    "        name=\"DQN_SimpleRL_vs_MaxBP\",\n",
    "        storage_path=os.path.abspath(os.path.join(os.getcwd(), os.pardir, 'results')),\n",
    "        stop=stop,\n",
    "        checkpoint_config=train.CheckpointConfig(\n",
    "            # checkpoint_frequency=4,\n",
    "            # checkpoint_score_attribute=\"episode_reward_mean\",\n",
    "            # num_to_keep=3,\n",
    "            checkpoint_at_end=True\n",
    "        ),\n",
    "    ),\n",
    "\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: free resources used by tune once training finished\n",
    "best_result = analysis.get_best_result(\n",
    "    metric=\"episode_reward_mean\", \n",
    "    mode=\"max\"\n",
    "    )\n",
    "# print(best_result)\n",
    "test_checkpoint = best_result.checkpoint\n",
    "\n",
    "# Load checkpoint from path\n",
    "# test_checkpoint = \"\"\n",
    "\n",
    "# Wait for training battles to finish closing before continuing to testing.\n",
    "# Without this, showdown gives a nametaken error because the players try to use the same names as in training.\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test algorithm against baseline players\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "# Against random player\n",
    "test_env_config = {\n",
    "    'username': 'te_SimpleRL',\n",
    "    'battle_format': \"gen9randombattle\",\n",
    "    'start_challenging': True,\n",
    "    'opponent_class': RandomPlayer,\n",
    "    'opponent_username': 'RandomPlayer',\n",
    "    'opponent_config': {\n",
    "        'battle_format': \"gen9randombattle\",\n",
    "    },\n",
    "}\n",
    "\n",
    "n_battles = 100\n",
    "# Restore the model checkpoint in test_checkpoint\n",
    "# TODO: stop .from_checkpoint from running env verification, it creates envs and does not close them properly.\n",
    "# Alternatively, make a callback that closes them.\n",
    "test_alg = Algorithm.from_checkpoint(test_checkpoint)\n",
    "# Create test environment.\n",
    "test_env = SimpleRLPlayer(env_config=test_env_config)\n",
    "\n",
    "for i in tqdm(range(n_battles), leave=False):\n",
    "    episode_reward = 0\n",
    "    terminated = truncated = False\n",
    "    obs, info = test_env.reset()\n",
    "    while not terminated and not truncated and not (test_env.current_battle is None) and not test_env.current_battle.finished:\n",
    "        action = test_alg.compute_single_action(obs)\n",
    "        obs, reward, terminated, truncated, info = test_env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "print(\"\\n\\nResults against random player:\")\n",
    "print(\n",
    "    f\"DQN Test: {test_env.n_won_battles} victories out of {test_env.n_finished_battles} episodes\"\n",
    ")\n",
    "test_env.close()\n",
    "\n",
    "# Against max base power player\n",
    "test_env_config = {\n",
    "    'username': 'te_SimpleRL',\n",
    "    'battle_format': \"gen9randombattle\",\n",
    "    'start_challenging': True,\n",
    "    'opponent_class': MaxBasePowerPlayer,\n",
    "    'opponent_username': 'MaxBasePower',\n",
    "    'opponent_config': {\n",
    "        'battle_format': \"gen9randombattle\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Create testing environment.\n",
    "test_env = SimpleRLPlayer(env_config=test_env_config)\n",
    "\n",
    "for i in tqdm(range(n_battles), leave=False):\n",
    "    episode_reward = 0\n",
    "    terminated = truncated = False\n",
    "    obs, info = test_env.reset()\n",
    "    while not terminated and not truncated and not (test_env.current_battle is None) and not test_env.current_battle.finished:\n",
    "        action = test_alg.compute_single_action(obs)\n",
    "        obs, reward, terminated, truncated, info = test_env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "print(\"\\nResults against Max Base Power player:\")\n",
    "print(\n",
    "    f\"DQN Test: {test_env.n_won_battles} victories out of {test_env.n_finished_battles} episodes\"\n",
    ")\n",
    "test_env.close()\n",
    "\n",
    "# Against SimpleHeuristics player\n",
    "test_env_config = {\n",
    "    'username': 'te_SimpleRL',\n",
    "    'battle_format': \"gen9randombattle\",\n",
    "    'start_challenging': True,\n",
    "    'opponent_class': SimpleHeuristicsPlayer,\n",
    "    'opponent_username': 'SimpleHeuristic',\n",
    "    'opponent_config': {\n",
    "        'battle_format': \"gen9randombattle\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Create testing environment.\n",
    "test_env = SimpleRLPlayer(env_config=test_env_config)\n",
    "\n",
    "for i in tqdm(range(n_battles), leave=False):\n",
    "    episode_reward = 0\n",
    "    terminated = truncated = False\n",
    "    obs, info = test_env.reset()\n",
    "    while not terminated and not truncated and not (test_env.current_battle is None) and not test_env.current_battle.finished:\n",
    "        action = test_alg.compute_single_action(obs)\n",
    "        obs, reward, terminated, truncated, info = test_env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "print(\"\\nResults against Simple Heuristic player:\")\n",
    "print(\n",
    "    f\"DQN Test: {test_env.n_won_battles} victories out of {test_env.n_finished_battles} episodes\"\n",
    ")\n",
    "test_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
