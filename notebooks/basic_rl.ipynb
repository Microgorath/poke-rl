{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Required Packages\n",
    "PyTorch or Tensorflow, preferably CUDA-enabled to utilize the GPU\\\n",
    "\"ray[rllib]\", requires Python 3.9 to 3.11. Note: ray[rllib] for Windows is currently in beta, and Windows version for Python 3.11 is experimental.\\\n",
    "poke-env ipython ipykernel ipywidgets tqdm tensorboard\n",
    "\n",
    "Note: RLLib is currently in the process of updating to 3.0, which changes to a new API stack. However, the new API is currently only available for PPO and SAC algorithms. This notebook currently uses the old API stack in order to use the DQN algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Local Pokemon Showdown Server\n",
    "cd into your pokemon-showdown directory  \n",
    "node pokemon-showdown start --no-security"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time\n",
    "from typing import List, Any, Union\n",
    "\n",
    "from gymnasium.spaces import Box, Space\n",
    "\n",
    "# from tabulate import tabulate\n",
    "\n",
    "from poke_env.environment.abstract_battle import AbstractBattle\n",
    "from poke_env.data import GenData\n",
    "type_chart = GenData(9).type_chart\n",
    "from poke_env.ps_client.account_configuration import AccountConfiguration, CONFIGURATION_FROM_PLAYER_COUNTER\n",
    "from poke_env.player import (\n",
    "    Gen9EnvSinglePlayer,\n",
    "    MaxBasePowerPlayer,\n",
    "    ObsType,\n",
    "    RandomPlayer,\n",
    "    SimpleHeuristicsPlayer\n",
    ")\n",
    "\n",
    "from ray.rllib.env.env_context import EnvContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def create_account_configuration(username: str, unique_ids: Union[List[Any], None] = None) -> AccountConfiguration:\n",
    "    # If no unique ids provided, create a unique id using the counter.\n",
    "    if unique_ids is None:\n",
    "        # NOTE: This is not thread-safe! Provide unique ids for multithreading / multi workers.\n",
    "        # The 's' in the resulting username indicates single thread.\n",
    "        CONFIGURATION_FROM_PLAYER_COUNTER.update([username])\n",
    "        unique_username = \"%s s %d\" % (username, CONFIGURATION_FROM_PLAYER_COUNTER[username])\n",
    "        if len(unique_username) > 18:\n",
    "            unique_username = \"%s s%d\" % (\n",
    "                username[: 18 - len(unique_username)],\n",
    "                CONFIGURATION_FROM_PLAYER_COUNTER[username],\n",
    "            )\n",
    "    else:\n",
    "        unique_ids_str = ' '.join(map(str, unique_ids))\n",
    "        unique_username = \"%s %s\" % (username, unique_ids_str)\n",
    "        unique_username = unique_username.strip()\n",
    "        if len(unique_username) > 18:\n",
    "            unique_username = \"%s %s\" % (\n",
    "                username[: 18 - len(unique_username)],\n",
    "                unique_ids_str,\n",
    "            )\n",
    "        \n",
    "    return AccountConfiguration(unique_username.strip(), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRLPlayer(Gen9EnvSinglePlayer):\n",
    "    def __init__(self, env_config: Union[EnvContext, dict]):\n",
    "        # Create list of the unique ids of the worker running this env.\n",
    "        if type(env_config) is EnvContext:\n",
    "            # The unique ids from EnvContext are the worker index, and the vector index.\n",
    "            # Worker index is index of the rollout worker that this env is running on, when there are multiple workers.\n",
    "            # Vector index is index of this env on this worker, when there are multiple envs per worker.\n",
    "            unique_ids = [env_config.worker_index, env_config.vector_index]\n",
    "        else:\n",
    "            # Set it to None. This activates create_account_configuration's player counter.\n",
    "            unique_ids = None\n",
    "\n",
    "        # Create unique account configuration for this from username to prevent multithreading naming conflicts\n",
    "        # account_configuration should be None, unless this is created as opponent for self-play\n",
    "        account_configuration = env_config.get('account_configuration')\n",
    "        if account_configuration is None:\n",
    "            # If no username provided, make one from this class name. \n",
    "            username =  env_config.get('username')\n",
    "            if username is None:\n",
    "                username = type(self).__name__\n",
    "            # Create unique account configuration from username, worker process id, and worker env id \n",
    "            account_configuration = create_account_configuration(username=username, unique_ids=unique_ids)\n",
    "        \n",
    "        # if opponent class provided, instantiate it with its config.\n",
    "        opponent_class = env_config.get('opponent_class')\n",
    "        if opponent_class is not None:\n",
    "            # Create unique opponent account configuration if none provided.\n",
    "            # opponent account_configuration should be None when using parallelization or multiple workers\n",
    "            opponent_account_configuration = env_config.get('opponent_account_configuration')\n",
    "            if opponent_account_configuration is None:\n",
    "                # If no opponent username provided, make one from its class name. \n",
    "                opponent_username =  env_config.get('opponent_username')\n",
    "                if opponent_username is None:\n",
    "                    opponent_username = type(opponent_class).__name__\n",
    "                # Create unique opponent account configuration from username, worker process index, and env index on this worker.\n",
    "                opponent_account_configuration = create_account_configuration(username=opponent_username, unique_ids=unique_ids)\n",
    "\n",
    "            # If opponent config provided, use it when instantiating opponent.\n",
    "            opponent_config = env_config.get('opponent_config')\n",
    "            if opponent_config is not None:\n",
    "                # Instantiate the opponent class with opponent config.\n",
    "                opponent = opponent_class(\n",
    "                    account_configuration = opponent_account_configuration,\n",
    "                    **opponent_config\n",
    "                )\n",
    "            else:\n",
    "                # If no opponent config provided, set battle format to be same as SimpleRLPlayer.\n",
    "                opponent = opponent_class(\n",
    "                    account_configuration = opponent_account_configuration, \n",
    "                    battle_format=env_config.get('battle_format')\n",
    "                )\n",
    "        else:\n",
    "            opponent = None\n",
    "        # TODO: Figure out how to only pass arguments if they are not None\n",
    "        super().__init__(\n",
    "            opponent = opponent,\n",
    "            account_configuration = account_configuration,\n",
    "            # avatar = env_config.get('avatar'),\n",
    "            # battle_format = env_config.get('battle_format'),\n",
    "            # log_level = env_config.get('log_level'),\n",
    "            # save_replays = env_config.get('save_replays'),\n",
    "            # server_configuration = env_config.get('server_configuration'),\n",
    "            # start_listening = env_config.get('start_listening'),\n",
    "            # start_timer_on_battle_start = env_config.get('start_timer_on_battle_start'),\n",
    "            # ping_interval = env_config.get('ping_interval'),\n",
    "            # ping_timeout = env_config.get('ping_timeout'),\n",
    "            # team = env_config.get('team'),\n",
    "            # start_challenging = env_config.get('start_challenging')\n",
    "        )\n",
    "        # Try adding this somewhere in init.\n",
    "        # self.reset_battles()\n",
    "\n",
    "\n",
    "    def calc_reward(self, last_battle, current_battle) -> float:\n",
    "        return self.reward_computing_helper(\n",
    "            current_battle, fainted_value=2.0, hp_value=1.0, victory_value=30.0\n",
    "        )\n",
    "\n",
    "    def embed_battle(self, battle: AbstractBattle) -> ObsType:\n",
    "        # -1 indicates that the move does not have a base power\n",
    "        # or is not available\n",
    "        moves_base_power = -np.ones(4)\n",
    "        moves_dmg_multiplier = np.ones(4)\n",
    "        for i, move in enumerate(battle.available_moves):\n",
    "            moves_base_power[i] = (\n",
    "                move.base_power / 100\n",
    "            )  # Simple rescaling to facilitate learning\n",
    "            if move.type:\n",
    "                moves_dmg_multiplier[i] = move.type.damage_multiplier(\n",
    "                    battle.opponent_active_pokemon.type_1,\n",
    "                    battle.opponent_active_pokemon.type_2,\n",
    "                    type_chart=type_chart\n",
    "                )\n",
    "\n",
    "        # We count how many pokemons have fainted in each team\n",
    "        fainted_mon_team = len([mon for mon in battle.team.values() if mon.fainted]) / 6\n",
    "        fainted_mon_opponent = (\n",
    "            len([mon for mon in battle.opponent_team.values() if mon.fainted]) / 6\n",
    "        )\n",
    "\n",
    "        # Final vector with 10 components\n",
    "        final_vector = np.concatenate(\n",
    "            [\n",
    "                moves_base_power,\n",
    "                moves_dmg_multiplier,\n",
    "                [fainted_mon_team, fainted_mon_opponent],\n",
    "            ]\n",
    "        )\n",
    "        return np.float32(final_vector)\n",
    "\n",
    "    def describe_embedding(self) -> Space:\n",
    "        low = [-1, -1, -1, -1, 0, 0, 0, 0, 0, 0]\n",
    "        high = [3, 3, 3, 3, 4, 4, 4, 4, 1, 1]\n",
    "        return Box(\n",
    "            np.array(low, dtype=np.float32),\n",
    "            np.array(high, dtype=np.float32),\n",
    "            dtype=np.float32,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create config\n",
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "from ray import tune, train\n",
    "import os\n",
    "\n",
    "# This is passed to each environment (SimpleRLPlayer) during training\n",
    "train_env_config = {\n",
    "    'username': 'tr_SimpleRL',\n",
    "    'battle_format': \"gen9randombattle\",\n",
    "    'start_challenging': True,\n",
    "    'opponent_class': MaxBasePowerPlayer,\n",
    "    'opponent_username': 'MaxBasePower',\n",
    "    'opponent_config': {\n",
    "        'battle_format': \"gen9randombattle\",\n",
    "    },\n",
    "}\n",
    "# This is passed to each environment (SimpleRLPlayer) during evaluation\n",
    "eval_env_config = {\n",
    "    'username': 'ev_SimpleRL',\n",
    "    'battle_format': \"gen9randombattle\",\n",
    "    'start_challenging': True,\n",
    "    'opponent_class': MaxBasePowerPlayer,\n",
    "    'opponent_username': 'MaxBasePower',\n",
    "    'opponent_config': {\n",
    "        'battle_format': \"gen9randombattle\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Guide to RLLib parameters: https://docs.ray.io/en/latest/rllib/rllib-training.html#common-parameters \n",
    "config = DQNConfig()\n",
    "config = config.environment(env = SimpleRLPlayer, env_config = train_env_config)\n",
    "config = config.resources(\n",
    "    num_gpus=1,\n",
    "    # num_learner_workers=8,\n",
    "    # num_cpus_per_learner_worker=2,\n",
    "    num_cpus_per_worker=1,\n",
    "    num_cpus_for_local_worker=2\n",
    ")\n",
    "config = config.env_runners(\n",
    "    # Number of workers to run environments. 0 forces rollouts onto the local worker.\n",
    "    num_env_runners=20,\n",
    "    num_envs_per_env_runner=1,\n",
    "    # Don't cut off episodes before they finish when batching.\n",
    "    # As a result, the batch size hyperparameter acts as a minimum and batches may vary in size.\n",
    "    # batch_mode=\"complete_episodes\",\n",
    "    # Validation creates environments and does not close them, causes problems.\n",
    "    # validate_env_runners_after_construction=False,\n",
    ")\n",
    "# Set training hyperparameters. \n",
    "# For descriptions, see: https://docs.ray.io/en/latest/rllib/rllib-algorithms.html#deep-q-networks-dqn-rainbow-parametric-dqn\n",
    "config = config.training(\n",
    "    # gamma=0.5,\n",
    "    lr=0.00025,\n",
    "    # lr=tune.loguniform(10**-6, 10**-4),\n",
    "    replay_buffer_config={\n",
    "        \"type\": \"MultiAgentPrioritizedReplayBuffer\",\n",
    "        \"capacity\": 50000,\n",
    "    },\n",
    "    num_steps_sampled_before_learning_starts=1000,\n",
    "    n_step=1,\n",
    "    double_q=True,\n",
    "    num_atoms=1,\n",
    "    noisy=False,\n",
    "    dueling=False,\n",
    "    # train_batch_size=tune.grid_search([4, 8, 12, 16, 20, 32, 64])\n",
    ")\n",
    "config = config.evaluation(\n",
    "    evaluation_interval=4,\n",
    "    # evaluation_num_workers=1,\n",
    "    # evaluation_parallel_to_training=True,\n",
    "    # evaluation_duration=\"auto\",\n",
    "    evaluation_config={\n",
    "        \"env_config\": eval_env_config,\n",
    "        \"metrics_num_episodes_for_smoothing\": 4,\n",
    "        # Set explore True for policy gradient algorithms\n",
    "        \"explore\": False,\n",
    "    },\n",
    ")\n",
    "# These settings allows runs to continue after a worker fails for whatever reason.\n",
    "# config.recreate_failed_env_runners = True\n",
    "\n",
    "stop = {\n",
    "    \"episode_reward_mean\": 30,\n",
    "    \"training_iteration\": 90,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-06-19 20:30:56</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:28.72        </td></tr>\n",
       "<tr><td>Memory:      </td><td>17.1/63.7 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 22.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 2<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                                                                    </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_SimpleRLPlayer_4af0a_00000</td><td style=\"text-align: right;\">           1</td><td>C:/Users/conno/AppData/Local/Temp/ray/session_2024-06-19_20-30-23_730255_37500/artifacts/2024-06-19_20-30-27/DQN_SimpleRL_Test/driver_artifacts/DQN_SimpleRLPlayer_4af0a_00000_0_2024-06-19_20-30-27/error.txt</td></tr>\n",
       "<tr><td>DQN_SimpleRLPlayer_4af0a_00001</td><td style=\"text-align: right;\">           1</td><td>C:/Users/conno/AppData/Local/Temp/ray/session_2024-06-19_20-30-23_730255_37500/artifacts/2024-06-19_20-30-27/DQN_SimpleRL_Test/driver_artifacts/DQN_SimpleRLPlayer_4af0a_00001_1_2024-06-19_20-30-27/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_SimpleRLPlayer_4af0a_00002</td><td>PENDING </td><td>     </td></tr>\n",
       "<tr><td>DQN_SimpleRLPlayer_4af0a_00003</td><td>PENDING </td><td>     </td></tr>\n",
       "<tr><td>DQN_SimpleRLPlayer_4af0a_00004</td><td>PENDING </td><td>     </td></tr>\n",
       "<tr><td>DQN_SimpleRLPlayer_4af0a_00005</td><td>PENDING </td><td>     </td></tr>\n",
       "<tr><td>DQN_SimpleRLPlayer_4af0a_00006</td><td>PENDING </td><td>     </td></tr>\n",
       "<tr><td>DQN_SimpleRLPlayer_4af0a_00007</td><td>PENDING </td><td>     </td></tr>\n",
       "<tr><td>DQN_SimpleRLPlayer_4af0a_00000</td><td>ERROR   </td><td>     </td></tr>\n",
       "<tr><td>DQN_SimpleRLPlayer_4af0a_00001</td><td>ERROR   </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(DQN pid=36376)\u001b[0m 2024-06-19 20:30:31,648\tWARNING deprecation.py:50 -- DeprecationWarning: `max_num_worker_restarts` has been deprecated. Use `AlgorithmConfig.max_num_env_runner_restarts` instead. This will raise an error in the future!\n",
      "\u001b[36m(DQN pid=36376)\u001b[0m 2024-06-19 20:30:40,125\tERROR actor_manager.py:517 -- Ray error, taking actor 1 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=47060, ip=127.0.0.1, actor_id=186a8641af0c1b16e0f89a3901000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000021B9330E470>)\n",
      "\u001b[36m(DQN pid=36376)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1887, in ray._raylet.execute_task\n",
      "\u001b[36m(DQN pid=36376)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1828, in ray._raylet.execute_task.function_executor\n",
      "\u001b[36m(DQN pid=36376)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 691, in actor_method_executor\n",
      "\u001b[36m(DQN pid=36376)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[36m(DQN pid=36376)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 467, in _resume_span\n",
      "\u001b[36m(DQN pid=36376)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(DQN pid=36376)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 532, in __init__\n",
      "\u001b[36m(DQN pid=36376)\u001b[0m     self._update_policy_map(policy_dict=self.policy_dict)\n",
      "\u001b[36m(DQN pid=36376)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 467, in _resume_span\n",
      "\u001b[36m(DQN pid=36376)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(DQN pid=36376)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1714, in _update_policy_map\n",
      "\u001b[36m(DQN pid=36376)\u001b[0m     updated_policy_dict = self._get_complete_policy_specs_dict(policy_dict)\n",
      "\u001b[36m(DQN pid=36376)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 467, in _resume_span\n",
      "\u001b[36m(DQN pid=36376)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(DQN pid=36376)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1792, in _get_complete_policy_specs_dict\n",
      "\u001b[36m(DQN pid=36376)\u001b[0m     preprocessor = ModelCatalog.get_preprocessor_for_space(\n",
      "\u001b[36m(DQN pid=36376)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\rllib\\models\\catalog.py\", line 895, in get_preprocessor_for_space\n",
      "\u001b[36m(DQN pid=36376)\u001b[0m     prep = cls(observation_space, options)\n",
      "\u001b[36m(DQN pid=36376)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\rllib\\models\\preprocessors.py\", line 41, in __init__\n",
      "\u001b[36m(DQN pid=36376)\u001b[0m     self._size = int(np.product(self.shape))\n",
      "\u001b[36m(DQN pid=36376)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\numpy\\__init__.py\", line 424, in __getattr__\n",
      "\u001b[36m(DQN pid=36376)\u001b[0m     raise AttributeError(\"module {!r} has no attribute \"\n",
      "\u001b[36m(DQN pid=36376)\u001b[0m AttributeError: module 'numpy' has no attribute 'product'\n",
      "\u001b[36m(DQN pid=36376)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::DQN.__init__()\u001b[39m (pid=36376, ip=127.0.0.1, actor_id=90bf3d76a6f12e649e61317201000000, repr=DQN)\n",
      "\u001b[36m(DQN pid=36376)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1887, in ray._raylet.execute_task\n",
      "\u001b[36m(DQN pid=36376)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1828, in ray._raylet.execute_task.function_executor\n",
      "\u001b[36m(DQN pid=36376)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 691, in actor_method_executor\n",
      "\u001b[36m(DQN pid=36376)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[36m(DQN pid=36376)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 467, in _resume_span\n",
      "\u001b[36m(DQN pid=36376)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(DQN pid=36376)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\", line 558, in __init__\n",
      "\u001b[36m(DQN pid=36376)\u001b[0m     super().__init__(\n",
      "\u001b[36m(DQN pid=36376)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\tune\\trainable\\trainable.py\", line 158, in __init__\n",
      "\u001b[36m(DQN pid=36376)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[36m(DQN pid=36376)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 467, in _resume_span\n",
      "\u001b[36m(DQN pid=36376)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(DQN pid=36376)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\", line 644, in setup\n",
      "\u001b[36m(DQN pid=36376)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[36m(DQN pid=36376)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 161, in __init__\n",
      "\u001b[36m(DQN pid=36376)\u001b[0m     self._setup(\n",
      "\u001b[36m(DQN pid=36376)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 246, in _setup\n",
      "\u001b[36m(DQN pid=36376)\u001b[0m     spaces = self._get_spaces_from_remote_worker()\n",
      "\u001b[36m(DQN pid=36376)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 302, in _get_spaces_from_remote_worker\n",
      "\u001b[36m(DQN pid=36376)\u001b[0m     raise ValueError(\n",
      "\u001b[36m(DQN pid=36376)\u001b[0m ValueError: Could not get observation and action spaces from remote worker. Maybe specify them manually in the config?\n",
      "\u001b[36m(RolloutWorker pid=47060)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=47060, ip=127.0.0.1, actor_id=186a8641af0c1b16e0f89a3901000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000021B9330E470>)\n",
      "2024-06-19 20:30:40,164\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_SimpleRLPlayer_4af0a_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\air\\execution\\_internal\\event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\_private\\auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\_private\\worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\_private\\worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died because of an error raised in its creation task, \u001b[36mray::DQN.__init__()\u001b[39m (pid=36376, ip=127.0.0.1, actor_id=90bf3d76a6f12e649e61317201000000, repr=DQN)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1887, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1828, in ray._raylet.execute_task.function_executor\n",
      "  File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 691, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 467, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\", line 558, in __init__\n",
      "    super().__init__(\n",
      "  File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\tune\\trainable\\trainable.py\", line 158, in __init__\n",
      "    self.setup(copy.deepcopy(self.config))\n",
      "  File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 467, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\", line 644, in setup\n",
      "    self.workers = WorkerSet(\n",
      "  File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 161, in __init__\n",
      "    self._setup(\n",
      "  File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 246, in _setup\n",
      "    spaces = self._get_spaces_from_remote_worker()\n",
      "  File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 302, in _get_spaces_from_remote_worker\n",
      "    raise ValueError(\n",
      "ValueError: Could not get observation and action spaces from remote worker. Maybe specify them manually in the config?\n",
      "\u001b[36m(DQN pid=50120)\u001b[0m 2024-06-19 20:30:44,512\tWARNING deprecation.py:50 -- DeprecationWarning: `max_num_worker_restarts` has been deprecated. Use `AlgorithmConfig.max_num_env_runner_restarts` instead. This will raise an error in the future!\n",
      "\u001b[36m(RolloutWorker pid=50380)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1887, in ray._raylet.execute_task\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=50380)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1828, in ray._raylet.execute_task.function_executor\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=50380)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 691, in actor_method_executor\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=50380)\u001b[0m     return method(__ray_actor, *args, **kwargs)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=50380)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 467, in _resume_span\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=50380)\u001b[0m     return method(self, *_args, **_kwargs)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=50380)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\rllib\\models\\preprocessors.py\", line 41, in __init__\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=50380)\u001b[0m     self._update_policy_map(policy_dict=self.policy_dict)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=50380)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1714, in _update_policy_map\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=50380)\u001b[0m     updated_policy_dict = self._get_complete_policy_specs_dict(policy_dict)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=50380)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1792, in _get_complete_policy_specs_dict\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=50380)\u001b[0m     preprocessor = ModelCatalog.get_preprocessor_for_space(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=50380)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\rllib\\models\\catalog.py\", line 895, in get_preprocessor_for_space\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=50380)\u001b[0m     prep = cls(observation_space, options)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=50380)\u001b[0m     self._size = int(np.product(self.shape))\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=50380)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\numpy\\__init__.py\", line 424, in __getattr__\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=50380)\u001b[0m     raise AttributeError(\"module {!r} has no attribute \"\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=50380)\u001b[0m AttributeError: module 'numpy' has no attribute 'product'\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=50380)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=50380, ip=127.0.0.1, actor_id=708180d0fa51416b7b0b570101000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000001B91335A470>)\n",
      "\u001b[36m(RolloutWorker pid=43100)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=43100, ip=127.0.0.1, actor_id=cab7f41236baa4018734338b01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000022E133BA470>)\n",
      "\u001b[36m(DQN pid=50120)\u001b[0m 2024-06-19 20:30:53,414\tERROR actor_manager.py:517 -- Ray error, taking actor 1 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=11104, ip=127.0.0.1, actor_id=3c8fda4d7e0f9c4e026bfa0901000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000019B144DA410>)\n",
      "\u001b[36m(DQN pid=50120)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::DQN.__init__()\u001b[39m (pid=50120, ip=127.0.0.1, actor_id=6ae8f73e0d5cf3fb288f33aa01000000, repr=DQN)\n",
      "\u001b[36m(DQN pid=50120)\u001b[0m     super().__init__(\n",
      "\u001b[36m(DQN pid=50120)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[36m(DQN pid=50120)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\", line 644, in setup\n",
      "\u001b[36m(DQN pid=50120)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[36m(DQN pid=50120)\u001b[0m     self._setup(\n",
      "\u001b[36m(DQN pid=50120)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 246, in _setup\n",
      "\u001b[36m(DQN pid=50120)\u001b[0m     spaces = self._get_spaces_from_remote_worker()\n",
      "\u001b[36m(DQN pid=50120)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 302, in _get_spaces_from_remote_worker\n",
      "\u001b[36m(DQN pid=50120)\u001b[0m     raise ValueError(\n",
      "\u001b[36m(DQN pid=50120)\u001b[0m ValueError: Could not get observation and action spaces from remote worker. Maybe specify them manually in the config?\n",
      "2024-06-19 20:30:53,448\tERROR tune_controller.py:1331 -- Trial task failed for trial DQN_SimpleRLPlayer_4af0a_00001\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\air\\execution\\_internal\\event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\_private\\auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\_private\\worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\_private\\worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died because of an error raised in its creation task, \u001b[36mray::DQN.__init__()\u001b[39m (pid=50120, ip=127.0.0.1, actor_id=6ae8f73e0d5cf3fb288f33aa01000000, repr=DQN)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1887, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1828, in ray._raylet.execute_task.function_executor\n",
      "  File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 691, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 467, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\", line 558, in __init__\n",
      "    super().__init__(\n",
      "  File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\tune\\trainable\\trainable.py\", line 158, in __init__\n",
      "    self.setup(copy.deepcopy(self.config))\n",
      "  File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 467, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\", line 644, in setup\n",
      "    self.workers = WorkerSet(\n",
      "  File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 161, in __init__\n",
      "    self._setup(\n",
      "  File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 246, in _setup\n",
      "    spaces = self._get_spaces_from_remote_worker()\n",
      "  File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 302, in _get_spaces_from_remote_worker\n",
      "    raise ValueError(\n",
      "ValueError: Could not get observation and action spaces from remote worker. Maybe specify them manually in the config?\n",
      "2024-06-19 20:30:56,614\tWARNING tune.py:219 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2024-06-19 20:30:56,622\tINFO tune.py:1007 -- Wrote the latest version of all result files and experiment state to 'c:/Users/conno/ML/Pokemon/poke-rl-test/results/DQN_SimpleRL_Test' in 0.0075s.\n",
      "\u001b[36m(DQN pid=8456)\u001b[0m 2024-06-19 20:30:57,896\tWARNING deprecation.py:50 -- DeprecationWarning: `max_num_worker_restarts` has been deprecated. Use `AlgorithmConfig.max_num_env_runner_restarts` instead. This will raise an error in the future!\n",
      "\u001b[36m(DQN pid=50120)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1887, in ray._raylet.execute_task\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN pid=50120)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1828, in ray._raylet.execute_task.function_executor\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN pid=50120)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 691, in actor_method_executor\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN pid=50120)\u001b[0m     return method(__ray_actor, *args, **kwargs)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN pid=50120)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 467, in _resume_span\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN pid=50120)\u001b[0m     return method(self, *_args, **_kwargs)\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN pid=50120)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 161, in __init__\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN pid=50120)\u001b[0m     self._update_policy_map(policy_dict=self.policy_dict)\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN pid=50120)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1714, in _update_policy_map\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN pid=50120)\u001b[0m     updated_policy_dict = self._get_complete_policy_specs_dict(policy_dict)\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN pid=50120)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1792, in _get_complete_policy_specs_dict\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN pid=50120)\u001b[0m     preprocessor = ModelCatalog.get_preprocessor_for_space(\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN pid=50120)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\rllib\\models\\catalog.py\", line 895, in get_preprocessor_for_space\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN pid=50120)\u001b[0m     prep = cls(observation_space, options)\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN pid=50120)\u001b[0m     self._size = int(np.product(self.shape))\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN pid=50120)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\numpy\\__init__.py\", line 424, in __getattr__\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN pid=50120)\u001b[0m     raise AttributeError(\"module {!r} has no attribute \"\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN pid=50120)\u001b[0m AttributeError: module 'numpy' has no attribute 'product'\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=38448)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=38448, ip=127.0.0.1, actor_id=d98b0015fcdefb2aa08260c101000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x00000286132BA440>)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=38448)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1887, in ray._raylet.execute_task\n",
      "\u001b[36m(RolloutWorker pid=38448)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1828, in ray._raylet.execute_task.function_executor\n",
      "\u001b[36m(RolloutWorker pid=38448)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 691, in actor_method_executor\n",
      "\u001b[36m(RolloutWorker pid=38448)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[36m(RolloutWorker pid=38448)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 467, in _resume_span\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=38448)\u001b[0m     return method(self, *_args, **_kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=38448)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\rllib\\models\\preprocessors.py\", line 41, in __init__\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=38448)\u001b[0m     self._update_policy_map(policy_dict=self.policy_dict)\n",
      "\u001b[36m(RolloutWorker pid=38448)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1714, in _update_policy_map\n",
      "\u001b[36m(RolloutWorker pid=38448)\u001b[0m     updated_policy_dict = self._get_complete_policy_specs_dict(policy_dict)\n",
      "\u001b[36m(RolloutWorker pid=38448)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1792, in _get_complete_policy_specs_dict\n",
      "\u001b[36m(RolloutWorker pid=38448)\u001b[0m     preprocessor = ModelCatalog.get_preprocessor_for_space(\n",
      "\u001b[36m(RolloutWorker pid=38448)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\rllib\\models\\catalog.py\", line 895, in get_preprocessor_for_space\n",
      "\u001b[36m(RolloutWorker pid=38448)\u001b[0m     prep = cls(observation_space, options)\n",
      "\u001b[36m(RolloutWorker pid=38448)\u001b[0m     self._size = int(np.product(self.shape))\n",
      "\u001b[36m(RolloutWorker pid=38448)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\numpy\\__init__.py\", line 424, in __getattr__\n",
      "\u001b[36m(RolloutWorker pid=38448)\u001b[0m     raise AttributeError(\"module {!r} has no attribute \"\n",
      "\u001b[36m(RolloutWorker pid=38448)\u001b[0m AttributeError: module 'numpy' has no attribute 'product'\n",
      "2024-06-19 20:31:06,748\tERROR tune.py:1035 -- Trials did not complete: [DQN_SimpleRLPlayer_4af0a_00000, DQN_SimpleRLPlayer_4af0a_00001]\n",
      "2024-06-19 20:31:06,749\tINFO tune.py:1039 -- Total run time: 38.87 seconds (28.71 seconds for the tuning loop).\n",
      "2024-06-19 20:31:06,750\tWARNING tune.py:1054 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: Tuner.restore(path=\"c:/Users/conno/ML/Pokemon/poke-rl-test/results/DQN_SimpleRL_Test\", trainable=...)\n",
      "2024-06-19 20:31:06,774\tWARNING experiment_analysis.py:180 -- Failed to fetch metrics for 8 trial(s):\n",
      "- DQN_SimpleRLPlayer_4af0a_00000: FileNotFoundError('Could not fetch metrics for DQN_SimpleRLPlayer_4af0a_00000: both result.json and progress.csv were not found at c:/Users/conno/ML/Pokemon/poke-rl-test/results/DQN_SimpleRL_Test/DQN_SimpleRLPlayer_4af0a_00000_0_2024-06-19_20-30-27')\n",
      "- DQN_SimpleRLPlayer_4af0a_00001: FileNotFoundError('Could not fetch metrics for DQN_SimpleRLPlayer_4af0a_00001: both result.json and progress.csv were not found at c:/Users/conno/ML/Pokemon/poke-rl-test/results/DQN_SimpleRL_Test/DQN_SimpleRLPlayer_4af0a_00001_1_2024-06-19_20-30-27')\n",
      "- DQN_SimpleRLPlayer_4af0a_00002: FileNotFoundError('Could not fetch metrics for DQN_SimpleRLPlayer_4af0a_00002: both result.json and progress.csv were not found at c:/Users/conno/ML/Pokemon/poke-rl-test/results/DQN_SimpleRL_Test/DQN_SimpleRLPlayer_4af0a_00002_2_2024-06-19_20-30-27')\n",
      "- DQN_SimpleRLPlayer_4af0a_00003: FileNotFoundError('Could not fetch metrics for DQN_SimpleRLPlayer_4af0a_00003: both result.json and progress.csv were not found at c:/Users/conno/ML/Pokemon/poke-rl-test/results/DQN_SimpleRL_Test/DQN_SimpleRLPlayer_4af0a_00003_3_2024-06-19_20-30-27')\n",
      "- DQN_SimpleRLPlayer_4af0a_00004: FileNotFoundError('Could not fetch metrics for DQN_SimpleRLPlayer_4af0a_00004: both result.json and progress.csv were not found at c:/Users/conno/ML/Pokemon/poke-rl-test/results/DQN_SimpleRL_Test/DQN_SimpleRLPlayer_4af0a_00004_4_2024-06-19_20-30-27')\n",
      "- DQN_SimpleRLPlayer_4af0a_00005: FileNotFoundError('Could not fetch metrics for DQN_SimpleRLPlayer_4af0a_00005: both result.json and progress.csv were not found at c:/Users/conno/ML/Pokemon/poke-rl-test/results/DQN_SimpleRL_Test/DQN_SimpleRLPlayer_4af0a_00005_5_2024-06-19_20-30-27')\n",
      "- DQN_SimpleRLPlayer_4af0a_00006: FileNotFoundError('Could not fetch metrics for DQN_SimpleRLPlayer_4af0a_00006: both result.json and progress.csv were not found at c:/Users/conno/ML/Pokemon/poke-rl-test/results/DQN_SimpleRL_Test/DQN_SimpleRLPlayer_4af0a_00006_6_2024-06-19_20-30-27')\n",
      "- DQN_SimpleRLPlayer_4af0a_00007: FileNotFoundError('Could not fetch metrics for DQN_SimpleRLPlayer_4af0a_00007: both result.json and progress.csv were not found at c:/Users/conno/ML/Pokemon/poke-rl-test/results/DQN_SimpleRL_Test/DQN_SimpleRLPlayer_4af0a_00007_7_2024-06-19_20-30-27')\n",
      "\u001b[36m(RolloutWorker pid=51492)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1887, in ray._raylet.execute_task\n",
      "\u001b[36m(RolloutWorker pid=51492)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1828, in ray._raylet.execute_task.function_executor\n",
      "\u001b[36m(RolloutWorker pid=51492)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 691, in actor_method_executor\n",
      "\u001b[36m(RolloutWorker pid=51492)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[36m(RolloutWorker pid=51492)\u001b[0m     self._update_policy_map(policy_dict=self.policy_dict)\n",
      "\u001b[36m(RolloutWorker pid=51492)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1714, in _update_policy_map\n",
      "\u001b[36m(RolloutWorker pid=51492)\u001b[0m     updated_policy_dict = self._get_complete_policy_specs_dict(policy_dict)\n",
      "\u001b[36m(RolloutWorker pid=51492)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1792, in _get_complete_policy_specs_dict\n",
      "\u001b[36m(RolloutWorker pid=51492)\u001b[0m     preprocessor = ModelCatalog.get_preprocessor_for_space(\n",
      "\u001b[36m(RolloutWorker pid=51492)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\rllib\\models\\catalog.py\", line 895, in get_preprocessor_for_space\n",
      "\u001b[36m(RolloutWorker pid=51492)\u001b[0m     prep = cls(observation_space, options)\n",
      "\u001b[36m(RolloutWorker pid=51492)\u001b[0m     self._size = int(np.product(self.shape))\n",
      "\u001b[36m(RolloutWorker pid=51492)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\numpy\\__init__.py\", line 424, in __getattr__\n",
      "\u001b[36m(RolloutWorker pid=51492)\u001b[0m     raise AttributeError(\"module {!r} has no attribute \"\n",
      "\u001b[36m(RolloutWorker pid=51492)\u001b[0m AttributeError: module 'numpy' has no attribute 'product'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RolloutWorker pid=20056)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=20056, ip=127.0.0.1, actor_id=c68221cbe1de285f43076ab801000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000001CF134EA410>)\n",
      "\u001b[36m(RolloutWorker pid=20056)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1887, in ray._raylet.execute_task\n",
      "\u001b[36m(RolloutWorker pid=20056)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 1828, in ray._raylet.execute_task.function_executor\n",
      "\u001b[36m(RolloutWorker pid=20056)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 691, in actor_method_executor\n",
      "\u001b[36m(RolloutWorker pid=20056)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[36m(RolloutWorker pid=20056)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 467, in _resume_span\n",
      "\u001b[36m(RolloutWorker pid=20056)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(RolloutWorker pid=20056)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 532, in __init__\n",
      "\u001b[36m(RolloutWorker pid=20056)\u001b[0m     self._update_policy_map(policy_dict=self.policy_dict)\n",
      "\u001b[36m(RolloutWorker pid=20056)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 467, in _resume_span\n",
      "\u001b[36m(RolloutWorker pid=20056)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(RolloutWorker pid=20056)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1714, in _update_policy_map\n",
      "\u001b[36m(RolloutWorker pid=20056)\u001b[0m     updated_policy_dict = self._get_complete_policy_specs_dict(policy_dict)\n",
      "\u001b[36m(RolloutWorker pid=20056)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 467, in _resume_span\n",
      "\u001b[36m(RolloutWorker pid=20056)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(RolloutWorker pid=20056)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1792, in _get_complete_policy_specs_dict\n",
      "\u001b[36m(RolloutWorker pid=20056)\u001b[0m     preprocessor = ModelCatalog.get_preprocessor_for_space(\n",
      "\u001b[36m(RolloutWorker pid=20056)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\rllib\\models\\catalog.py\", line 895, in get_preprocessor_for_space\n",
      "\u001b[36m(RolloutWorker pid=20056)\u001b[0m     prep = cls(observation_space, options)\n",
      "\u001b[36m(RolloutWorker pid=20056)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\rllib\\models\\preprocessors.py\", line 41, in __init__\n",
      "\u001b[36m(RolloutWorker pid=20056)\u001b[0m     self._size = int(np.product(self.shape))\n",
      "\u001b[36m(RolloutWorker pid=20056)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\numpy\\__init__.py\", line 424, in __getattr__\n",
      "\u001b[36m(RolloutWorker pid=20056)\u001b[0m     raise AttributeError(\"module {!r} has no attribute \"\n",
      "\u001b[36m(RolloutWorker pid=20056)\u001b[0m AttributeError: module 'numpy' has no attribute 'product'\n",
      "\u001b[36m(RolloutWorker pid=20056)\u001b[0m Stack (most recent call first):\n",
      "\u001b[36m(RolloutWorker pid=20056)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\_private\\worker.py\", line 876 in main_loop\n",
      "\u001b[36m(RolloutWorker pid=20056)\u001b[0m   File \"c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\_private\\workers\\default_worker.py\", line 289 in <module>\n"
     ]
    }
   ],
   "source": [
    "# Auto hyperparameter tuning\n",
    "\n",
    "\n",
    "analysis = tune.Tuner(\n",
    "    config.algo_class,\n",
    "    param_space=config,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=\"episode_reward_mean\",\n",
    "        mode=\"max\",\n",
    "        num_samples=8,\n",
    "        # reuse_actors=True, # Needs reset_config() to be defined and return True for this algorithm\n",
    "    ),\n",
    "    run_config=train.RunConfig(\n",
    "        name=\"DQN_SimpleRL_Test\",\n",
    "        storage_path=os.path.abspath(os.path.join(os.getcwd(), os.pardir, 'results')),\n",
    "        stop=stop,\n",
    "        checkpoint_config=train.CheckpointConfig(\n",
    "            checkpoint_frequency=4,\n",
    "            checkpoint_score_attribute=\"episode_reward_mean\",\n",
    "            num_to_keep=3,\n",
    "            checkpoint_at_end=True\n",
    "        ),\n",
    "    ),\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 20:29:22,727\tWARNING experiment_analysis.py:558 -- Could not find best trial. Did you pass the correct `metric` parameter?\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No best trial found for the given metric: episode_reward_mean. This means that no trial has reported this metric, or all values reported for this metric are NaN. To not ignore NaN values, you can set the `filter_nan_and_inf` arg to False.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# TODO: free resources used by tune once training finished\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m best_result \u001b[38;5;241m=\u001b[39m \u001b[43manalysis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_best_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;43;03m#     # metric=\"env_runners/episode_return_mean\", \u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;43;03m#     # mode=\"max\"\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m test_checkpoint \u001b[38;5;241m=\u001b[39m best_result\u001b[38;5;241m.\u001b[39mcheckpoint\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Load checkpoint from path\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# test_checkpoint = \u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Wait for training battles to finish closing before continuing to testing.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Without this, showdown gives a nametaken error because the players try to use the same names as in training.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\tune\\result_grid.py:161\u001b[0m, in \u001b[0;36mResultGrid.get_best_result\u001b[1;34m(self, metric, mode, scope, filter_nan_and_inf)\u001b[0m\n\u001b[0;32m    150\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    151\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo best trial found for the given metric: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    152\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experiment_analysis\u001b[38;5;241m.\u001b[39mdefault_metric\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    153\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis means that no trial has reported this metric\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    154\u001b[0m     )\n\u001b[0;32m    155\u001b[0m     error_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, or all values reported for this metric are NaN. To not ignore NaN \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues, you can set the `filter_nan_and_inf` arg to False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m filter_nan_and_inf\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m     )\n\u001b[1;32m--> 161\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(error_msg)\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trial_to_result(best_trial)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: No best trial found for the given metric: episode_reward_mean. This means that no trial has reported this metric, or all values reported for this metric are NaN. To not ignore NaN values, you can set the `filter_nan_and_inf` arg to False."
     ]
    }
   ],
   "source": [
    "# TODO: free resources used by tune once training finished\n",
    "best_result = analysis.get_best_result(\n",
    "#     # metric=\"env_runners/episode_return_mean\", \n",
    "#     # mode=\"max\"\n",
    "    )\n",
    "test_checkpoint = best_result.checkpoint\n",
    "\n",
    "# Load checkpoint from path\n",
    "# test_checkpoint = \n",
    "\n",
    "# Wait for training battles to finish closing before continuing to testing.\n",
    "# Without this, showdown gives a nametaken error because the players try to use the same names as in training.\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 01:58:41,766\tWARNING deprecation.py:50 -- DeprecationWarning: `rllib/algorithms/simple_q/` has been deprecated. Use `rllib_contrib/simple_q/` instead. This will raise an error in the future!\n",
      "c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:500: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\tune\\logger\\unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\tune\\logger\\unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "c:\\Users\\conno\\anaconda3\\envs\\poke-rl-rllib\\lib\\site-packages\\ray\\tune\\logger\\unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2024-06-19 01:58:43,677\tINFO worker.py:1752 -- Started a local Ray instance.\n",
      "2024-06-19 01:59:02,396\tINFO trainable.py:164 -- Trainable.setup took 20.501 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2024-06-19 01:59:02,397\tWARNING util.py:62 -- Install gputil for GPU system monitoring.\n",
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Results against random player:\n",
      "DQN Test: 97 victories out of 100 episodes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results against Max Base Power player:\n",
      "DQN Test: 66 victories out of 100 episodes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results against Simple Heuristic player:\n",
      "DQN Test: 25 victories out of 100 episodes\n"
     ]
    }
   ],
   "source": [
    "## Test algorithm against baseline players\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "# Against random player\n",
    "test_env_config = {\n",
    "    'username': 'te_SimpleRL',\n",
    "    'battle_format': \"gen9randombattle\",\n",
    "    'start_challenging': True,\n",
    "    'opponent_class': RandomPlayer,\n",
    "    'opponent_username': 'RandomPlayer',\n",
    "    'opponent_config': {\n",
    "        'battle_format': \"gen9randombattle\",\n",
    "    },\n",
    "}\n",
    "\n",
    "n_battles = 100\n",
    "# Restore the model checkpoint in test_checkpoint\n",
    "# TODO: stop .from_checkpoint from running env verification, it creates envs and does not close them properly.\n",
    "# Alternatively, make a callback that closes them.\n",
    "test_alg = Algorithm.from_checkpoint(test_checkpoint)\n",
    "# Create test environment.\n",
    "test_env = SimpleRLPlayer(env_config=test_env_config)\n",
    "\n",
    "for i in tqdm(range(n_battles), leave=False):\n",
    "    episode_reward = 0\n",
    "    terminated = truncated = False\n",
    "    obs, info = test_env.reset()\n",
    "    while not terminated and not truncated and not (test_env.current_battle is None) and not test_env.current_battle.finished:\n",
    "        action = test_alg.compute_single_action(obs)\n",
    "        obs, reward, terminated, truncated, info = test_env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "print(\"\\n\\nResults against random player:\")\n",
    "print(\n",
    "    f\"DQN Test: {test_env.n_won_battles} victories out of {test_env.n_finished_battles} episodes\"\n",
    ")\n",
    "test_env.close()\n",
    "\n",
    "# Against max base power player\n",
    "test_env_config = {\n",
    "    'username': 'te_SimpleRL',\n",
    "    'battle_format': \"gen9randombattle\",\n",
    "    'start_challenging': True,\n",
    "    'opponent_class': MaxBasePowerPlayer,\n",
    "    'opponent_username': 'MaxBasePower',\n",
    "    'opponent_config': {\n",
    "        'battle_format': \"gen9randombattle\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Create testing environment.\n",
    "test_env = SimpleRLPlayer(env_config=test_env_config)\n",
    "\n",
    "for i in tqdm(range(n_battles), leave=False):\n",
    "    episode_reward = 0\n",
    "    terminated = truncated = False\n",
    "    obs, info = test_env.reset()\n",
    "    while not terminated and not truncated and not (test_env.current_battle is None) and not test_env.current_battle.finished:\n",
    "        action = test_alg.compute_single_action(obs)\n",
    "        obs, reward, terminated, truncated, info = test_env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "print(\"\\nResults against Max Base Power player:\")\n",
    "print(\n",
    "    f\"DQN Test: {test_env.n_won_battles} victories out of {test_env.n_finished_battles} episodes\"\n",
    ")\n",
    "test_env.close()\n",
    "\n",
    "# Against SimpleHeuristics player\n",
    "test_env_config = {\n",
    "    'username': 'te_SimpleRL',\n",
    "    'battle_format': \"gen9randombattle\",\n",
    "    'start_challenging': True,\n",
    "    'opponent_class': SimpleHeuristicsPlayer,\n",
    "    'opponent_username': 'SimpleHeuristic',\n",
    "    'opponent_config': {\n",
    "        'battle_format': \"gen9randombattle\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Create testing environment.\n",
    "test_env = SimpleRLPlayer(env_config=test_env_config)\n",
    "\n",
    "for i in tqdm(range(n_battles), leave=False):\n",
    "    episode_reward = 0\n",
    "    terminated = truncated = False\n",
    "    obs, info = test_env.reset()\n",
    "    while not terminated and not truncated and not (test_env.current_battle is None) and not test_env.current_battle.finished:\n",
    "        action = test_alg.compute_single_action(obs)\n",
    "        obs, reward, terminated, truncated, info = test_env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "print(\"\\nResults against Simple Heuristic player:\")\n",
    "print(\n",
    "    f\"DQN Test: {test_env.n_won_battles} victories out of {test_env.n_finished_battles} episodes\"\n",
    ")\n",
    "test_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poke-rl-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
