{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time\n",
    "\n",
    "from gymnasium.spaces import Box, Space\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "from poke_env.environment.abstract_battle import AbstractBattle\n",
    "from poke_env.data import GenData\n",
    "type_chart = GenData(9).type_chart\n",
    "from poke_env.ps_client.account_configuration import AccountConfiguration, CONFIGURATION_FROM_PLAYER_COUNTER\n",
    "\n",
    "from poke_env.player import (\n",
    "    Gen9EnvSinglePlayer,\n",
    "    MaxBasePowerPlayer,\n",
    "    ObsType,\n",
    "    RandomPlayer,\n",
    "    SimpleHeuristicsPlayer,\n",
    "    background_cross_evaluate,\n",
    "    background_evaluate_player,\n",
    ")\n",
    "from ray.rllib.env.env_context import EnvContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "from typing import List, Any, Union\n",
    "def create_account_configuration(username: str, unique_ids: Union[List[Any], None] = None) -> AccountConfiguration:\n",
    "    # If no unique ids provided, create a unique id using the counter.\n",
    "    if unique_ids is None:\n",
    "        # NOTE: This is not thread-safe! Provide unique ids for multithreading / multi workers.\n",
    "        # The 's' in the resulting username indicates single thread.\n",
    "        CONFIGURATION_FROM_PLAYER_COUNTER.update([username])\n",
    "        unique_username = \"%s s %d\" % (username, CONFIGURATION_FROM_PLAYER_COUNTER[username])\n",
    "        if len(unique_username) > 18:\n",
    "            unique_username = \"%s s%d\" % (\n",
    "                username[: 18 - len(unique_username)],\n",
    "                CONFIGURATION_FROM_PLAYER_COUNTER[username],\n",
    "            )\n",
    "    else:\n",
    "        unique_ids_str = ' '.join(map(str, unique_ids))\n",
    "        unique_username = \"%s %s\" % (username, unique_ids_str)\n",
    "        unique_username = unique_username.strip()\n",
    "        if len(unique_username) > 18:\n",
    "            unique_username = \"%s %s\" % (\n",
    "                username[: 18 - len(unique_username)],\n",
    "                unique_ids_str,\n",
    "            )\n",
    "        \n",
    "    return AccountConfiguration(unique_username.strip(), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRLPlayer(Gen9EnvSinglePlayer):\n",
    "    def __init__(self, env_config: Union[EnvContext, dict]):\n",
    "        # Create list of the unique ids of the worker running this env.\n",
    "        if type(env_config) is EnvContext:\n",
    "            # The unique ids from EnvContext are the worker index, and the vector index.\n",
    "            # Worker index is index of the rollout worker that this env is running on, when there are multiple workers.\n",
    "            # Vector index is index of this env on this worker, when there are multiple envs per worker.\n",
    "            unique_ids = [env_config.worker_index, env_config.vector_index]\n",
    "        else:\n",
    "            # Set it to None. This activates create_account_configuration's player counter.\n",
    "            unique_ids = None\n",
    "\n",
    "        # Create unique account configuration for this from username to prevent multithreading naming conflicts\n",
    "        # account_configuration should be None, unless this is created as opponent for self-play\n",
    "        account_configuration = env_config.get('account_configuration')\n",
    "        if account_configuration is None:\n",
    "            # If no username provided, make one from this class name. \n",
    "            username =  env_config.get('username')\n",
    "            if username is None:\n",
    "                username = type(self).__name__\n",
    "            # Create unique account configuration from username, worker process id, and worker env id \n",
    "            account_configuration = create_account_configuration(username=username, unique_ids=unique_ids)\n",
    "        \n",
    "        # if opponent class provided, instantiate it with its config.\n",
    "        opponent_class = env_config.get('opponent_class')\n",
    "        if opponent_class is not None:\n",
    "            # Create unique opponent account configuration if none provided.\n",
    "            # opponent account_configuration should be None when using parallelization or multiple workers\n",
    "            opponent_account_configuration = env_config.get('opponent_account_configuration')\n",
    "            if opponent_account_configuration is None:\n",
    "                # If no opponent username provided, make one from its class name. \n",
    "                opponent_username =  env_config.get('opponent_username')\n",
    "                if opponent_username is None:\n",
    "                    opponent_username = type(opponent_class).__name__\n",
    "                # Create unique opponent account configuration from username, worker process index, and env index on this worker.\n",
    "                opponent_account_configuration = create_account_configuration(username=opponent_username, unique_ids=unique_ids)\n",
    "\n",
    "            # If opponent config provided, use it when instantiating opponent.\n",
    "            opponent_config = env_config.get('opponent_config')\n",
    "            if opponent_config is not None:\n",
    "                # Instantiate the opponent class with opponent config.\n",
    "                opponent = opponent_class(\n",
    "                    account_configuration = opponent_account_configuration,\n",
    "                    **opponent_config\n",
    "                )\n",
    "            else:\n",
    "                # If no opponent config provided, set battle format to be same as SimpleRLPlayer.\n",
    "                opponent = opponent_class(\n",
    "                    account_configuration = opponent_account_configuration, \n",
    "                    battle_format=env_config.get('battle_format')\n",
    "                )\n",
    "        else:\n",
    "            opponent = None\n",
    "        # TODO: Figure out how to only pass arguments if they are not None\n",
    "        super().__init__(\n",
    "            opponent = opponent,\n",
    "            account_configuration = account_configuration,\n",
    "            # avatar = env_config.get('avatar'),\n",
    "            # battle_format = env_config.get('battle_format'),\n",
    "            # log_level = env_config.get('log_level'),\n",
    "            # save_replays = env_config.get('save_replays'),\n",
    "            # server_configuration = env_config.get('server_configuration'),\n",
    "            # start_listening = env_config.get('start_listening'),\n",
    "            # start_timer_on_battle_start = env_config.get('start_timer_on_battle_start'),\n",
    "            # ping_interval = env_config.get('ping_interval'),\n",
    "            # ping_timeout = env_config.get('ping_timeout'),\n",
    "            # team = env_config.get('team'),\n",
    "            # start_challenging = env_config.get('start_challenging')\n",
    "        )\n",
    "        # Try adding this.\n",
    "        # self.reset_battles()\n",
    "\n",
    "\n",
    "    def calc_reward(self, last_battle, current_battle) -> float:\n",
    "        return self.reward_computing_helper(\n",
    "            current_battle, fainted_value=2.0, hp_value=1.0, victory_value=30.0\n",
    "        )\n",
    "\n",
    "    def embed_battle(self, battle: AbstractBattle) -> ObsType:\n",
    "        # -1 indicates that the move does not have a base power\n",
    "        # or is not available\n",
    "        moves_base_power = -np.ones(4)\n",
    "        moves_dmg_multiplier = np.ones(4)\n",
    "        for i, move in enumerate(battle.available_moves):\n",
    "            moves_base_power[i] = (\n",
    "                move.base_power / 100\n",
    "            )  # Simple rescaling to facilitate learning\n",
    "            if move.type:\n",
    "                moves_dmg_multiplier[i] = move.type.damage_multiplier(\n",
    "                    battle.opponent_active_pokemon.type_1,\n",
    "                    battle.opponent_active_pokemon.type_2,\n",
    "                    type_chart=type_chart\n",
    "                )\n",
    "\n",
    "        # We count how many pokemons have fainted in each team\n",
    "        fainted_mon_team = len([mon for mon in battle.team.values() if mon.fainted]) / 6\n",
    "        fainted_mon_opponent = (\n",
    "            len([mon for mon in battle.opponent_team.values() if mon.fainted]) / 6\n",
    "        )\n",
    "\n",
    "        # Final vector with 10 components\n",
    "        final_vector = np.concatenate(\n",
    "            [\n",
    "                moves_base_power,\n",
    "                moves_dmg_multiplier,\n",
    "                [fainted_mon_team, fainted_mon_opponent],\n",
    "            ]\n",
    "        )\n",
    "        return np.float32(final_vector)\n",
    "\n",
    "    def describe_embedding(self) -> Space:\n",
    "        low = [-1, -1, -1, -1, 0, 0, 0, 0, 0, 0]\n",
    "        high = [3, 3, 3, 3, 4, 4, 4, 4, 1, 1]\n",
    "        return Box(\n",
    "            np.array(low, dtype=np.float32),\n",
    "            np.array(high, dtype=np.float32),\n",
    "            dtype=np.float32,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create config\n",
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "from ray import tune, train\n",
    "import os\n",
    "\n",
    "# This is passed to each environment (SimpleRLPlayer) during training\n",
    "train_env_config = {\n",
    "    'username': 'tr_SimpleRL',\n",
    "    'battle_format': \"gen9randombattle\",\n",
    "    'start_challenging': True,\n",
    "    'opponent_class': RandomPlayer,\n",
    "    'opponent_username': 'RandomPlayer',\n",
    "    'opponent_config': {\n",
    "        'battle_format': \"gen9randombattle\",\n",
    "    },\n",
    "}\n",
    "# This is passed to each environment (SimpleRLPlayer) during evaluation\n",
    "eval_env_config = {\n",
    "    'username': 'ev_SimpleRL',\n",
    "    'battle_format': \"gen9randombattle\",\n",
    "    'start_challenging': True,\n",
    "    'opponent_class': RandomPlayer,\n",
    "    'opponent_username': 'RandomPlayer',\n",
    "    'opponent_config': {\n",
    "        'battle_format': \"gen9randombattle\",\n",
    "    },\n",
    "}\n",
    "\n",
    "config = DQNConfig()\n",
    "config = config.environment(env = SimpleRLPlayer, env_config = train_env_config, disable_env_checking=True)\n",
    "config = config.resources(\n",
    "    num_gpus=1,\n",
    "    # num_learner_workers=8,\n",
    "    # num_cpus_per_learner_worker=2,\n",
    "    num_cpus_per_worker=1,\n",
    "    num_cpus_for_local_worker=2\n",
    ")\n",
    "config = config.rollouts(\n",
    "    num_rollout_workers=4, # 0 forces rollouts onto the local worker.\n",
    "    num_envs_per_worker=1,\n",
    "    validate_workers_after_construction=False, # Validation creates environments and does not close them, causes problems.\n",
    ")\n",
    "config = config.training(\n",
    "    lr=tune.grid_search([10**-3, 10**-5]),\n",
    "    replay_buffer_config={\n",
    "        \"type\": \"MultiAgentPrioritizedReplayBuffer\",\n",
    "        \"capacity\": 10000,\n",
    "    },\n",
    "    num_steps_sampled_before_learning_starts=500,\n",
    "    n_step=1,\n",
    "    double_q=True,\n",
    "    num_atoms=1,\n",
    "    noisy=False,\n",
    "    dueling=False,\n",
    ")\n",
    "config = config.evaluation(\n",
    "    evaluation_interval=5,\n",
    "    # evaluation_num_workers=1,\n",
    "    # evaluation_parallel_to_training=True,\n",
    "    # evaluation_duration=\"auto\",\n",
    "    evaluation_config={\n",
    "        \"env_config\": eval_env_config,\n",
    "        \"metrics_num_episodes_for_smoothing\": 4,\n",
    "        # Set explore True for policy gradient algorithms\n",
    "        \"explore\": False,\n",
    "    },\n",
    ")\n",
    "\n",
    "stop = {\n",
    "    \"episode_reward_mean\": 30,\n",
    "    \"training_iteration\": 60,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto hyperparameter tuning\n",
    "\n",
    "\n",
    "analysis = tune.Tuner(\n",
    "    config.algo_class,\n",
    "    param_space=config,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=\"episode_reward_mean\",\n",
    "        mode=\"max\",\n",
    "        num_samples=8,\n",
    "        reuse_actors=True,\n",
    "        max_concurrent_trials=4\n",
    "    ),\n",
    "    run_config=train.RunConfig(\n",
    "        name=\"DQN_SimpleRL_Test\",\n",
    "        storage_path=os.path.abspath(os.path.join(os.getcwd(), os.pardir, 'results')),\n",
    "        stop=stop,\n",
    "        checkpoint_config=train.CheckpointConfig(\n",
    "            checkpoint_frequency=4,\n",
    "            num_to_keep=10,\n",
    "            checkpoint_at_end=True\n",
    "        ),\n",
    "    ),\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result = analysis.get_best_result(metric=\"env_runners/episode_return_mean\", mode=\"max\")\n",
    "test_checkpoint = best_result.checkpoint\n",
    "# Wait for training battles to finish closing before continuing to testing.\n",
    "# Without this, showdown gives a nametaken error because the players try to use the same names as in training.\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test algorithm against baseline players\n",
    "# Against random player\n",
    "test_env_config = {\n",
    "    'username': 'te_SimpleRL',\n",
    "    'battle_format': \"gen9randombattle\",\n",
    "    'start_challenging': True,\n",
    "    'opponent_class': RandomPlayer,\n",
    "    'opponent_username': 'RandomPlayer',\n",
    "    'opponent_config': {\n",
    "        'battle_format': \"gen9randombattle\",\n",
    "    },\n",
    "}\n",
    "\n",
    "n_battles = 100\n",
    "# Restore the model checkpoint in test_checkpoint\n",
    "# TODO: stop .from_checkpoint from running env verification, it creates envs and does not close them properly.\n",
    "# Alternatively, make a callback that closes them.\n",
    "test_alg = Algorithm.from_checkpoint(test_checkpoint)\n",
    "# Create test environment.\n",
    "test_env = SimpleRLPlayer(env_config=test_env_config)\n",
    "\n",
    "for i in tqdm(range(n_battles), leave=False):\n",
    "    episode_reward = 0\n",
    "    terminated = truncated = False\n",
    "    obs, info = test_env.reset()\n",
    "    while not terminated and not truncated and not (test_env.current_battle is None) and not test_env.current_battle.finished:\n",
    "        action = test_alg.compute_single_action(obs)\n",
    "        obs, reward, terminated, truncated, info = test_env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "print(\"\\n\\nResults against random player:\")\n",
    "print(\n",
    "    f\"DQN Test: {test_env.n_won_battles} victories out of {test_env.n_finished_battles} episodes\"\n",
    ")\n",
    "\n",
    "# Against max base power player\n",
    "test_env_config = {\n",
    "    'username': 'te_SimpleRL',\n",
    "    'battle_format': \"gen9randombattle\",\n",
    "    'start_challenging': True,\n",
    "    'opponent_class': MaxBasePowerPlayer,\n",
    "    'opponent_username': 'MaxBP',\n",
    "    'opponent_config': {\n",
    "        'battle_format': \"gen9randombattle\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Create testing environment.\n",
    "test_env = SimpleRLPlayer(env_config=test_env_config)\n",
    "\n",
    "for i in tqdm(range(n_battles), leave=False):\n",
    "    episode_reward = 0\n",
    "    terminated = truncated = False\n",
    "    obs, info = test_env.reset()\n",
    "    while not terminated and not truncated and not (test_env.current_battle is None) and not test_env.current_battle.finished:\n",
    "        action = test_alg.compute_single_action(obs)\n",
    "        obs, reward, terminated, truncated, info = test_env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "print(\"\\nResults against Max Base Power player:\")\n",
    "print(\n",
    "    f\"DQN Test: {test_env.n_won_battles} victories out of {test_env.n_finished_battles} episodes\"\n",
    ")\n",
    "test_env.reset_env(restart=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the player with included util method\n",
    "# Random player\n",
    "# eval_env_config = {\n",
    "#     'opponent': RandomPlayer,\n",
    "#     'battle_format': \"gen9randombattle\",\n",
    "#     'start_challenging': True,\n",
    "# }\n",
    "# eval_env = SimpleRLPlayer(eval_env_config)\n",
    "\n",
    "\n",
    "# n_challenges = 10\n",
    "# placement_battles = 4\n",
    "# eval_task = background_evaluate_player(\n",
    "#     eval_env.agent, n_challenges, placement_battles\n",
    "# )\n",
    "\n",
    "# # Make a new config for evaluation from the train config\n",
    "# eval_config = alg.config.copy(copy_frozen=False)\n",
    "# # Increase duration of evaluation\n",
    "# eval_config.evaluation_duration = n_challenges\n",
    "# eval_config.evaluation_interval = 1\n",
    "# eval_config.env = eval_env\n",
    "# alg.reset_config(eval_config) # Unsure if this resets weights, I want to reset everything but that.\n",
    "\n",
    "# alg.evaluate()\n",
    "# print(\"Evaluation with included method:\", eval_task.result())\n",
    "# eval_env.reset_env(restart=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross evaluate the player with included util method\n",
    "# n_challenges = 50\n",
    "# players = [\n",
    "#     eval_env.agent,\n",
    "#     RandomPlayer(battle_format=\"gen9randombattle\"),\n",
    "#     MaxBasePowerPlayer(battle_format=\"gen9randombattle\"),\n",
    "#     SimpleHeuristicsPlayer(battle_format=\"gen9randombattle\"),\n",
    "# ]\n",
    "# cross_eval_task = background_cross_evaluate(players, n_challenges)\n",
    "# dqn.test(\n",
    "#     eval_env,\n",
    "#     nb_episodes=n_challenges * (len(players) - 1),\n",
    "#     verbose=False,\n",
    "#     visualize=False,\n",
    "# )\n",
    "# cross_evaluation = cross_eval_task.result()\n",
    "# table = [[\"-\"] + [p.username for p in players]]\n",
    "# for p_1, results in cross_evaluation.items():\n",
    "#     table.append([p_1] + [cross_evaluation[p_1][p_2] for p_2 in results])\n",
    "# print(\"Cross evaluation of DQN with baselines:\")\n",
    "# print(tabulate(table))\n",
    "# eval_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poke-rl-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
