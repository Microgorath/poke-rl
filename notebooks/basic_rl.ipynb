{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Required Packages\n",
    "PyTorch or Tensorflow, preferably CUDA-enabled to utilize the GPU\\\n",
    "\"ray[rllib]\", requires Python 3.9 to 3.11. Note: ray[rllib] for Windows is currently in beta, and Windows version for Python 3.11 is experimental.\\\n",
    "poke-env ipython ipykernel ipywidgets tqdm tensorboard\n",
    "\n",
    "Note: RLLib is currently in the process of updating to 3.0, which changes to a new API stack. However, the new API is currently only available for PPO and SAC algorithms. This notebook currently uses the old API stack in order to use the DQN algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Local Pokemon Showdown Server\n",
    "cd into your pokemon-showdown directory  \n",
    "node pokemon-showdown start --no-security"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time\n",
    "from typing import List, Any, Union\n",
    "\n",
    "from gymnasium.spaces import Box, Space\n",
    "\n",
    "# from tabulate import tabulate\n",
    "\n",
    "from poke_env.environment.abstract_battle import AbstractBattle\n",
    "from poke_env.data import GenData\n",
    "type_chart = GenData(9).type_chart\n",
    "from poke_env.ps_client.account_configuration import AccountConfiguration, CONFIGURATION_FROM_PLAYER_COUNTER\n",
    "from poke_env.player import (\n",
    "    Gen9EnvSinglePlayer,\n",
    "    MaxBasePowerPlayer,\n",
    "    ObsType,\n",
    "    RandomPlayer,\n",
    "    SimpleHeuristicsPlayer\n",
    ")\n",
    "\n",
    "from ray.rllib.env.env_context import EnvContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def create_account_configuration(username: str, unique_ids: Union[List[Any], None] = None) -> AccountConfiguration:\n",
    "    # If no unique ids provided, create a unique id using the counter.\n",
    "    if unique_ids is None:\n",
    "        # NOTE: This is not thread-safe! Provide unique ids for multithreading / multi workers.\n",
    "        # The 's' in the resulting username indicates single thread.\n",
    "        CONFIGURATION_FROM_PLAYER_COUNTER.update([username])\n",
    "        unique_username = \"%s s %d\" % (username, CONFIGURATION_FROM_PLAYER_COUNTER[username])\n",
    "        if len(unique_username) > 18:\n",
    "            unique_username = \"%s s%d\" % (\n",
    "                username[: 18 - len(unique_username)],\n",
    "                CONFIGURATION_FROM_PLAYER_COUNTER[username],\n",
    "            )\n",
    "    else:\n",
    "        unique_ids_str = ' '.join(map(str, unique_ids))\n",
    "        unique_username = \"%s %s\" % (username, unique_ids_str)\n",
    "        unique_username = unique_username.strip()\n",
    "        if len(unique_username) > 18:\n",
    "            unique_username = \"%s %s\" % (\n",
    "                username[: 18 - len(unique_username)],\n",
    "                unique_ids_str,\n",
    "            )\n",
    "        \n",
    "    return AccountConfiguration(unique_username.strip(), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRLPlayer(Gen9EnvSinglePlayer):\n",
    "    def __init__(self, env_config: Union[EnvContext, dict]):\n",
    "        # Create list of the unique ids of the worker running this env.\n",
    "        if type(env_config) is EnvContext:\n",
    "            # The unique ids from EnvContext are the worker index, and the vector index.\n",
    "            # Worker index is index of the rollout worker that this env is running on, when there are multiple workers.\n",
    "            # Vector index is index of this env on this worker, when there are multiple envs per worker.\n",
    "            unique_ids = [env_config.worker_index, env_config.vector_index]\n",
    "        else:\n",
    "            # Set it to None. This activates create_account_configuration's player counter.\n",
    "            unique_ids = None\n",
    "\n",
    "        # Create unique account configuration for this from username to prevent multithreading naming conflicts\n",
    "        # account_configuration should be None, unless this is created as opponent for self-play\n",
    "        account_configuration = env_config.get('account_configuration')\n",
    "        if account_configuration is None:\n",
    "            # If no username provided, make one from this class name. \n",
    "            username =  env_config.get('username')\n",
    "            if username is None:\n",
    "                username = type(self).__name__\n",
    "            # Create unique account configuration from username, worker process id, and worker env id \n",
    "            account_configuration = create_account_configuration(username=username, unique_ids=unique_ids)\n",
    "        \n",
    "        # if opponent class provided, instantiate it with its config.\n",
    "        opponent_class = env_config.get('opponent_class')\n",
    "        if opponent_class is not None:\n",
    "            # Create unique opponent account configuration if none provided.\n",
    "            # opponent account_configuration should be None when using parallelization or multiple workers\n",
    "            opponent_account_configuration = env_config.get('opponent_account_configuration')\n",
    "            if opponent_account_configuration is None:\n",
    "                # If no opponent username provided, make one from its class name. \n",
    "                opponent_username =  env_config.get('opponent_username')\n",
    "                if opponent_username is None:\n",
    "                    opponent_username = type(opponent_class).__name__\n",
    "                # Create unique opponent account configuration from username, worker process index, and env index on this worker.\n",
    "                opponent_account_configuration = create_account_configuration(username=opponent_username, unique_ids=unique_ids)\n",
    "\n",
    "            # If opponent config provided, use it when instantiating opponent.\n",
    "            opponent_config = env_config.get('opponent_config')\n",
    "            if opponent_config is not None:\n",
    "                # Instantiate the opponent class with opponent config.\n",
    "                opponent = opponent_class(\n",
    "                    account_configuration = opponent_account_configuration,\n",
    "                    **opponent_config\n",
    "                )\n",
    "            else:\n",
    "                # If no opponent config provided, set battle format to be same as SimpleRLPlayer.\n",
    "                opponent = opponent_class(\n",
    "                    account_configuration = opponent_account_configuration, \n",
    "                    battle_format=env_config.get('battle_format')\n",
    "                )\n",
    "        else:\n",
    "            opponent = None\n",
    "        # TODO: Figure out how to only pass arguments if they are not None\n",
    "        super().__init__(\n",
    "            opponent = opponent,\n",
    "            account_configuration = account_configuration,\n",
    "            # avatar = env_config.get('avatar'),\n",
    "            # battle_format = env_config.get('battle_format'),\n",
    "            # log_level = env_config.get('log_level'),\n",
    "            # save_replays = env_config.get('save_replays'),\n",
    "            # server_configuration = env_config.get('server_configuration'),\n",
    "            # start_listening = env_config.get('start_listening'),\n",
    "            # start_timer_on_battle_start = env_config.get('start_timer_on_battle_start'),\n",
    "            # ping_interval = env_config.get('ping_interval'),\n",
    "            # ping_timeout = env_config.get('ping_timeout'),\n",
    "            # team = env_config.get('team'),\n",
    "            # start_challenging = env_config.get('start_challenging')\n",
    "        )\n",
    "        # Try adding this somewhere in init.\n",
    "        # self.reset_battles()\n",
    "\n",
    "\n",
    "    def calc_reward(self, last_battle, current_battle) -> float:\n",
    "        return self.reward_computing_helper(\n",
    "            current_battle, fainted_value=2.0, hp_value=1.0, victory_value=30.0\n",
    "        )\n",
    "\n",
    "    def embed_battle(self, battle: AbstractBattle) -> ObsType:\n",
    "        # -1 indicates that the move does not have a base power\n",
    "        # or is not available\n",
    "        moves_base_power = -np.ones(4)\n",
    "        moves_dmg_multiplier = np.ones(4)\n",
    "        for i, move in enumerate(battle.available_moves):\n",
    "            moves_base_power[i] = (\n",
    "                move.base_power / 100\n",
    "            )  # Simple rescaling to facilitate learning\n",
    "            if move.type:\n",
    "                moves_dmg_multiplier[i] = move.type.damage_multiplier(\n",
    "                    battle.opponent_active_pokemon.type_1,\n",
    "                    battle.opponent_active_pokemon.type_2,\n",
    "                    type_chart=type_chart\n",
    "                )\n",
    "\n",
    "        # We count how many pokemons have fainted in each team\n",
    "        fainted_mon_team = len([mon for mon in battle.team.values() if mon.fainted]) / 6\n",
    "        fainted_mon_opponent = (\n",
    "            len([mon for mon in battle.opponent_team.values() if mon.fainted]) / 6\n",
    "        )\n",
    "\n",
    "        # Final vector with 10 components\n",
    "        final_vector = np.concatenate(\n",
    "            [\n",
    "                moves_base_power,\n",
    "                moves_dmg_multiplier,\n",
    "                [fainted_mon_team, fainted_mon_opponent],\n",
    "            ]\n",
    "        )\n",
    "        return np.float32(final_vector)\n",
    "\n",
    "    def describe_embedding(self) -> Space:\n",
    "        low = [-1, -1, -1, -1, 0, 0, 0, 0, 0, 0]\n",
    "        high = [3, 3, 3, 3, 4, 4, 4, 4, 1, 1]\n",
    "        return Box(\n",
    "            np.array(low, dtype=np.float32),\n",
    "            np.array(high, dtype=np.float32),\n",
    "            dtype=np.float32,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create config\n",
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "from ray import tune, train\n",
    "import os\n",
    "\n",
    "# This is passed to each environment (SimpleRLPlayer) during training\n",
    "train_env_config = {\n",
    "    'username': 'tr_SimpleRL',\n",
    "    'battle_format': \"gen9randombattle\",\n",
    "    'start_challenging': True,\n",
    "    'opponent_class': MaxBasePowerPlayer,\n",
    "    'opponent_username': 'MaxBasePower',\n",
    "    'opponent_config': {\n",
    "        'battle_format': \"gen9randombattle\",\n",
    "    },\n",
    "}\n",
    "# This is passed to each environment (SimpleRLPlayer) during evaluation\n",
    "eval_env_config = {\n",
    "    'username': 'ev_SimpleRL',\n",
    "    'battle_format': \"gen9randombattle\",\n",
    "    'start_challenging': True,\n",
    "    'opponent_class': MaxBasePowerPlayer,\n",
    "    'opponent_username': 'MaxBasePower',\n",
    "    'opponent_config': {\n",
    "        'battle_format': \"gen9randombattle\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Guide to RLLib parameters: https://docs.ray.io/en/latest/rllib/rllib-training.html#common-parameters \n",
    "config = DQNConfig()\n",
    "config = config.environment(env = SimpleRLPlayer, env_config = train_env_config)\n",
    "config = config.resources(\n",
    "    num_gpus=1,\n",
    "    # num_learner_workers=8,\n",
    "    # num_cpus_per_learner_worker=2,\n",
    "    num_cpus_per_worker=1,\n",
    "    num_cpus_for_local_worker=2\n",
    ")\n",
    "config = config.env_runners(\n",
    "    # Number of workers to run environments. 0 forces rollouts onto the local worker.\n",
    "    num_env_runners=20,\n",
    "    num_envs_per_env_runner=1,\n",
    "    # Don't cut off episodes before they finish when batching.\n",
    "    # As a result, the batch size hyperparameter acts as a minimum and batches may vary in size.\n",
    "    # batch_mode=\"complete_episodes\",\n",
    "    # Validation creates environments and does not close them, causes problems.\n",
    "    # validate_env_runners_after_construction=False,\n",
    ")\n",
    "# Set training hyperparameters. \n",
    "# For descriptions, see: https://docs.ray.io/en/latest/rllib/rllib-algorithms.html#deep-q-networks-dqn-rainbow-parametric-dqn\n",
    "config = config.training(\n",
    "    # gamma=0.5,\n",
    "    lr=0.00025,\n",
    "    # lr=tune.loguniform(10**-6, 10**-4),\n",
    "    replay_buffer_config={\n",
    "        \"type\": \"MultiAgentPrioritizedReplayBuffer\",\n",
    "        \"capacity\": 50000,\n",
    "    },\n",
    "    num_steps_sampled_before_learning_starts=1000,\n",
    "    n_step=1,\n",
    "    double_q=True,\n",
    "    num_atoms=1,\n",
    "    noisy=False,\n",
    "    dueling=False,\n",
    "    # train_batch_size=tune.grid_search([4, 8, 12, 16, 20, 32, 64])\n",
    ")\n",
    "config = config.evaluation(\n",
    "    evaluation_interval=4,\n",
    "    # evaluation_num_workers=1,\n",
    "    # evaluation_parallel_to_training=True,\n",
    "    # evaluation_duration=\"auto\",\n",
    "    evaluation_config={\n",
    "        \"env_config\": eval_env_config,\n",
    "        \"metrics_num_episodes_for_smoothing\": 4,\n",
    "        # Set explore True for policy gradient algorithms\n",
    "        \"explore\": False,\n",
    "    },\n",
    ")\n",
    "# These settings allows runs to continue after a worker fails for whatever reason.\n",
    "# config.recreate_failed_env_runners = True\n",
    "\n",
    "stop = {\n",
    "    \"episode_reward_mean\": 30,\n",
    "    \"training_iteration\": 90,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto hyperparameter tuning\n",
    "\n",
    "\n",
    "analysis = tune.Tuner(\n",
    "    config.algo_class,\n",
    "    param_space=config,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=\"episode_reward_mean\",\n",
    "        mode=\"max\",\n",
    "        num_samples=8,\n",
    "        # reuse_actors=True, # Needs reset_config() to be defined and return True for this algorithm\n",
    "    ),\n",
    "    run_config=train.RunConfig(\n",
    "        name=\"DQN_SimpleRL_Test\",\n",
    "        storage_path=os.path.abspath(os.path.join(os.getcwd(), os.pardir, 'results')),\n",
    "        stop=stop,\n",
    "        checkpoint_config=train.CheckpointConfig(\n",
    "            checkpoint_frequency=4,\n",
    "            checkpoint_score_attribute=\"episode_reward_mean\",\n",
    "            num_to_keep=3,\n",
    "            checkpoint_at_end=True\n",
    "        ),\n",
    "    ),\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: free resources used by tune once training finished\n",
    "best_result = analysis.get_best_result(\n",
    "#     # metric=\"env_runners/episode_return_mean\", \n",
    "#     # mode=\"max\"\n",
    "    )\n",
    "test_checkpoint = best_result.checkpoint\n",
    "\n",
    "# Load checkpoint from path\n",
    "# test_checkpoint = \n",
    "\n",
    "# Wait for training battles to finish closing before continuing to testing.\n",
    "# Without this, showdown gives a nametaken error because the players try to use the same names as in training.\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test algorithm against baseline players\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "# Against random player\n",
    "test_env_config = {\n",
    "    'username': 'te_SimpleRL',\n",
    "    'battle_format': \"gen9randombattle\",\n",
    "    'start_challenging': True,\n",
    "    'opponent_class': RandomPlayer,\n",
    "    'opponent_username': 'RandomPlayer',\n",
    "    'opponent_config': {\n",
    "        'battle_format': \"gen9randombattle\",\n",
    "    },\n",
    "}\n",
    "\n",
    "n_battles = 100\n",
    "# Restore the model checkpoint in test_checkpoint\n",
    "# TODO: stop .from_checkpoint from running env verification, it creates envs and does not close them properly.\n",
    "# Alternatively, make a callback that closes them.\n",
    "test_alg = Algorithm.from_checkpoint(test_checkpoint)\n",
    "# Create test environment.\n",
    "test_env = SimpleRLPlayer(env_config=test_env_config)\n",
    "\n",
    "for i in tqdm(range(n_battles), leave=False):\n",
    "    episode_reward = 0\n",
    "    terminated = truncated = False\n",
    "    obs, info = test_env.reset()\n",
    "    while not terminated and not truncated and not (test_env.current_battle is None) and not test_env.current_battle.finished:\n",
    "        action = test_alg.compute_single_action(obs)\n",
    "        obs, reward, terminated, truncated, info = test_env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "print(\"\\n\\nResults against random player:\")\n",
    "print(\n",
    "    f\"DQN Test: {test_env.n_won_battles} victories out of {test_env.n_finished_battles} episodes\"\n",
    ")\n",
    "test_env.close()\n",
    "\n",
    "# Against max base power player\n",
    "test_env_config = {\n",
    "    'username': 'te_SimpleRL',\n",
    "    'battle_format': \"gen9randombattle\",\n",
    "    'start_challenging': True,\n",
    "    'opponent_class': MaxBasePowerPlayer,\n",
    "    'opponent_username': 'MaxBasePower',\n",
    "    'opponent_config': {\n",
    "        'battle_format': \"gen9randombattle\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Create testing environment.\n",
    "test_env = SimpleRLPlayer(env_config=test_env_config)\n",
    "\n",
    "for i in tqdm(range(n_battles), leave=False):\n",
    "    episode_reward = 0\n",
    "    terminated = truncated = False\n",
    "    obs, info = test_env.reset()\n",
    "    while not terminated and not truncated and not (test_env.current_battle is None) and not test_env.current_battle.finished:\n",
    "        action = test_alg.compute_single_action(obs)\n",
    "        obs, reward, terminated, truncated, info = test_env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "print(\"\\nResults against Max Base Power player:\")\n",
    "print(\n",
    "    f\"DQN Test: {test_env.n_won_battles} victories out of {test_env.n_finished_battles} episodes\"\n",
    ")\n",
    "test_env.close()\n",
    "\n",
    "# Against SimpleHeuristics player\n",
    "test_env_config = {\n",
    "    'username': 'te_SimpleRL',\n",
    "    'battle_format': \"gen9randombattle\",\n",
    "    'start_challenging': True,\n",
    "    'opponent_class': SimpleHeuristicsPlayer,\n",
    "    'opponent_username': 'SimpleHeuristic',\n",
    "    'opponent_config': {\n",
    "        'battle_format': \"gen9randombattle\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Create testing environment.\n",
    "test_env = SimpleRLPlayer(env_config=test_env_config)\n",
    "\n",
    "for i in tqdm(range(n_battles), leave=False):\n",
    "    episode_reward = 0\n",
    "    terminated = truncated = False\n",
    "    obs, info = test_env.reset()\n",
    "    while not terminated and not truncated and not (test_env.current_battle is None) and not test_env.current_battle.finished:\n",
    "        action = test_alg.compute_single_action(obs)\n",
    "        obs, reward, terminated, truncated, info = test_env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "print(\"\\nResults against Simple Heuristic player:\")\n",
    "print(\n",
    "    f\"DQN Test: {test_env.n_won_battles} victories out of {test_env.n_finished_battles} episodes\"\n",
    ")\n",
    "test_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poke-rl-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
