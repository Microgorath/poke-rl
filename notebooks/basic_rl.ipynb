{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from gymnasium.spaces import Box, Space\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "from poke_env.environment.abstract_battle import AbstractBattle\n",
    "from poke_env.data import GenData\n",
    "type_chart = GenData(9).type_chart\n",
    "\n",
    "from poke_env.player import (\n",
    "    Gen9EnvSinglePlayer,\n",
    "    MaxBasePowerPlayer,\n",
    "    ObsType,\n",
    "    RandomPlayer,\n",
    "    SimpleHeuristicsPlayer,\n",
    "    background_cross_evaluate,\n",
    "    background_evaluate_player,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRLPlayer(Gen9EnvSinglePlayer):\n",
    "    def __init__(self, env_config):\n",
    "        self.env_config = env_config.copy()\n",
    "        opponent_class = self.env_config.get('opponent')\n",
    "        # TODO: add full opponent config to env_config, for now:\n",
    "        # if opponent provided, set it to the same battle format.\n",
    "        if opponent_class is not None:\n",
    "            # instantiate the opponent class before passing it to the superclass\n",
    "            self.env_config['opponent'] = opponent_class(battle_format=env_config.get('battle_format'))\n",
    "\n",
    "        super().__init__(**self.env_config)\n",
    "\n",
    "\n",
    "    def calc_reward(self, last_battle, current_battle) -> float:\n",
    "        return self.reward_computing_helper(\n",
    "            current_battle, fainted_value=2.0, hp_value=1.0, victory_value=30.0\n",
    "        )\n",
    "\n",
    "    def embed_battle(self, battle: AbstractBattle) -> ObsType:\n",
    "        # -1 indicates that the move does not have a base power\n",
    "        # or is not available\n",
    "        moves_base_power = -np.ones(4)\n",
    "        moves_dmg_multiplier = np.ones(4)\n",
    "        for i, move in enumerate(battle.available_moves):\n",
    "            moves_base_power[i] = (\n",
    "                move.base_power / 100\n",
    "            )  # Simple rescaling to facilitate learning\n",
    "            if move.type:\n",
    "                moves_dmg_multiplier[i] = move.type.damage_multiplier(\n",
    "                    battle.opponent_active_pokemon.type_1,\n",
    "                    battle.opponent_active_pokemon.type_2,\n",
    "                    type_chart=type_chart\n",
    "                )\n",
    "\n",
    "        # We count how many pokemons have fainted in each team\n",
    "        fainted_mon_team = len([mon for mon in battle.team.values() if mon.fainted]) / 6\n",
    "        fainted_mon_opponent = (\n",
    "            len([mon for mon in battle.opponent_team.values() if mon.fainted]) / 6\n",
    "        )\n",
    "\n",
    "        # Final vector with 10 components\n",
    "        final_vector = np.concatenate(\n",
    "            [\n",
    "                moves_base_power,\n",
    "                moves_dmg_multiplier,\n",
    "                [fainted_mon_team, fainted_mon_opponent],\n",
    "            ]\n",
    "        )\n",
    "        return np.float32(final_vector)\n",
    "\n",
    "    def describe_embedding(self) -> Space:\n",
    "        low = [-1, -1, -1, -1, 0, 0, 0, 0, 0, 0]\n",
    "        high = [3, 3, 3, 3, 4, 4, 4, 4, 1, 1]\n",
    "        return Box(\n",
    "            np.array(low, dtype=np.float32),\n",
    "            np.array(high, dtype=np.float32),\n",
    "            dtype=np.float32,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create config\n",
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "\n",
    "\n",
    "# This is passed to each environment (SimpleRLPlayer) during training\n",
    "train_env_config = {\n",
    "    'opponent': RandomPlayer,\n",
    "    'battle_format': \"gen9randombattle\",\n",
    "    'start_challenging': True,\n",
    "}\n",
    "# This is passed to each environment (SimpleRLPlayer) during evaluation\n",
    "eval_env_config = {\n",
    "    'opponent': RandomPlayer,\n",
    "    'battle_format': \"gen9randombattle\",\n",
    "    'start_challenging': True\n",
    "}\n",
    "\n",
    "config = DQNConfig()\n",
    "config = config.environment(env = SimpleRLPlayer, env_config = train_env_config, disable_env_checking=True)\n",
    "config = config.training(\n",
    "    # train_batch_size_per_learner=1,\n",
    "    replay_buffer_config={\n",
    "        \"type\": \"MultiAgentPrioritizedReplayBuffer\",\n",
    "        \"capacity\": 10000,\n",
    "    },\n",
    "    num_steps_sampled_before_learning_starts=500,\n",
    "    n_step=1,\n",
    "    double_q=True,\n",
    "    num_atoms=1,\n",
    "    noisy=False,\n",
    "    dueling=False,\n",
    ")\n",
    "# config = config.learners(\n",
    "#     num_learners=1, # set to number of GPUs\n",
    "#     num_gpus_per_learner=1, # set to 1, if have at least 1 gpu\n",
    "# )\n",
    "# config = config.env_runners()\n",
    "# config = config.rollouts()\n",
    "config = config.evaluation(\n",
    "    evaluation_interval=5,\n",
    "    # evaluation_num_workers=1,\n",
    "    # evaluation_parallel_to_training=True,\n",
    "    # evaluation_duration=\"auto\",\n",
    "    evaluation_config={\n",
    "        \"env_config\": eval_env_config,\n",
    "        \"metrics_num_episodes_for_smoothing\": 4,\n",
    "        # Set explore True for policy gradient algorithms\n",
    "        \"explore\": False,\n",
    "    },\n",
    ")\n",
    "\n",
    "stop = {\n",
    "    \"training_iteration\": 50\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto hyperparameter tuning\n",
    "from ray import tune, train\n",
    "import os\n",
    "\n",
    "analysis = tune.Tuner(\n",
    "    config.algo_class,\n",
    "    param_space=config,\n",
    "    run_config=train.RunConfig(\n",
    "        name=\"DQN_SimpleRL_Test\",\n",
    "        storage_path=os.path.abspath(os.path.join(os.getcwd(), os.pardir, 'results')),\n",
    "        stop=stop,\n",
    "        checkpoint_config=train.CheckpointConfig(\n",
    "            checkpoint_frequency=1,\n",
    "            num_to_keep=10,\n",
    "            checkpoint_at_end=True\n",
    "        )\n",
    "    ),\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result = analysis.get_best_result(metric=\"env_runners/episode_return_mean\", mode=\"max\")\n",
    "eval_checkpoint = best_result.checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate algorithm\n",
    "# Against random player\n",
    "eval_env_config = {\n",
    "    'opponent': RandomPlayer,\n",
    "    'battle_format': \"gen9randombattle\",\n",
    "    'start_challenging': True\n",
    "}\n",
    "\n",
    "n_battles = 100\n",
    "# Restore the model checkpoint in eval_checkpoint\n",
    "# TODO: stop .from_checkpoint from running env verification, it creates envs and does not close them properly.\n",
    "# Alternatively, make a callback that closes them.\n",
    "eval_alg = Algorithm.from_checkpoint(eval_checkpoint)\n",
    "# Create evaluation environment.\n",
    "eval_env = SimpleRLPlayer(env_config=eval_env_config)\n",
    "\n",
    "for i in tqdm(range(n_battles), leave=False):\n",
    "    episode_reward = 0\n",
    "    terminated = truncated = False\n",
    "    obs, info = eval_env.reset()\n",
    "    while not terminated and not truncated and not (eval_env.current_battle is None) and not eval_env.current_battle.finished:\n",
    "        action = eval_alg.compute_single_action(obs)\n",
    "        obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "print(\"\\n\\nResults against random player:\")\n",
    "print(\n",
    "    f\"DQN Evaluation: {eval_env.n_won_battles} victories out of {eval_env.n_finished_battles} episodes\"\n",
    ")\n",
    "\n",
    "# Against max base power player\n",
    "eval_env_config = {\n",
    "    'opponent': MaxBasePowerPlayer,\n",
    "    'battle_format': \"gen9randombattle\",\n",
    "    'start_challenging': True\n",
    "}\n",
    "\n",
    "# Create evaluation environment.\n",
    "eval_env = SimpleRLPlayer(env_config=eval_env_config)\n",
    "\n",
    "for i in tqdm(range(n_battles), leave=False):\n",
    "    episode_reward = 0\n",
    "    terminated = truncated = False\n",
    "    obs, info = eval_env.reset()\n",
    "    while not terminated and not truncated and not (eval_env.current_battle is None) and not eval_env.current_battle.finished:\n",
    "        action = eval_alg.compute_single_action(obs)\n",
    "        obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "print(\"\\nResults against Max Base Power player:\")\n",
    "print(\n",
    "    f\"DQN Evaluation: {eval_env.n_won_battles} victories out of {eval_env.n_finished_battles} episodes\"\n",
    ")\n",
    "eval_env.reset_env(restart=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the player with included util method\n",
    "# Random player\n",
    "# eval_env_config = {\n",
    "#     'opponent': RandomPlayer,\n",
    "#     'battle_format': \"gen9randombattle\",\n",
    "#     'start_challenging': True,\n",
    "# }\n",
    "# eval_env = SimpleRLPlayer(eval_env_config)\n",
    "\n",
    "\n",
    "# n_challenges = 10\n",
    "# placement_battles = 4\n",
    "# eval_task = background_evaluate_player(\n",
    "#     eval_env.agent, n_challenges, placement_battles\n",
    "# )\n",
    "\n",
    "# # Make a new config for evaluation from the train config\n",
    "# eval_config = alg.config.copy(copy_frozen=False)\n",
    "# # Increase duration of evaluation\n",
    "# eval_config.evaluation_duration = n_challenges\n",
    "# eval_config.evaluation_interval = 1\n",
    "# eval_config.env = eval_env\n",
    "# alg.reset_config(eval_config) # Unsure if this resets weights, I want to reset everything but that.\n",
    "\n",
    "# alg.evaluate()\n",
    "# print(\"Evaluation with included method:\", eval_task.result())\n",
    "# eval_env.reset_env(restart=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross evaluate the player with included util method\n",
    "# n_challenges = 50\n",
    "# players = [\n",
    "#     eval_env.agent,\n",
    "#     RandomPlayer(battle_format=\"gen9randombattle\"),\n",
    "#     MaxBasePowerPlayer(battle_format=\"gen9randombattle\"),\n",
    "#     SimpleHeuristicsPlayer(battle_format=\"gen9randombattle\"),\n",
    "# ]\n",
    "# cross_eval_task = background_cross_evaluate(players, n_challenges)\n",
    "# dqn.test(\n",
    "#     eval_env,\n",
    "#     nb_episodes=n_challenges * (len(players) - 1),\n",
    "#     verbose=False,\n",
    "#     visualize=False,\n",
    "# )\n",
    "# cross_evaluation = cross_eval_task.result()\n",
    "# table = [[\"-\"] + [p.username for p in players]]\n",
    "# for p_1, results in cross_evaluation.items():\n",
    "#     table.append([p_1] + [cross_evaluation[p_1][p_2] for p_2 in results])\n",
    "# print(\"Cross evaluation of DQN with baselines:\")\n",
    "# print(tabulate(table))\n",
    "# eval_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poke-rl-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
