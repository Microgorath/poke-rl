{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook's code is from https://github.com/hsahovic/poke-env/blob/master/examples/rl_with_new_open_ai_gym_wrapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from gymnasium.spaces import Box, Space\n",
    "from gymnasium.utils.env_checker import check_env\n",
    "# from gymnasium.envs.registration import register\n",
    "\n",
    "from openrl.modules.common import DQNNet\n",
    "from openrl.runners.common import DQNAgent\n",
    "from openrl.envs.common import make\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "from poke_env.environment.abstract_battle import AbstractBattle\n",
    "from poke_env.data import GenData\n",
    "type_chart = GenData(8).type_chart\n",
    "\n",
    "from poke_env.player import (\n",
    "    Gen8EnvSinglePlayer,\n",
    "    MaxBasePowerPlayer,\n",
    "    ObsType,\n",
    "    RandomPlayer,\n",
    "    SimpleHeuristicsPlayer,\n",
    "    background_cross_evaluate,\n",
    "    background_evaluate_player,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRLPlayer(Gen8EnvSinglePlayer):\n",
    "    agent_num = 1 # necessary for OpenRL's environment instantiation\n",
    "    \n",
    "    def calc_reward(self, last_battle, current_battle) -> float:\n",
    "        return self.reward_computing_helper(\n",
    "            current_battle, fainted_value=2.0, hp_value=1.0, victory_value=30.0\n",
    "        )\n",
    "\n",
    "    def embed_battle(self, battle: AbstractBattle) -> ObsType:\n",
    "        # -1 indicates that the move does not have a base power\n",
    "        # or is not available\n",
    "        moves_base_power = -np.ones(4)\n",
    "        moves_dmg_multiplier = np.ones(4)\n",
    "        for i, move in enumerate(battle.available_moves):\n",
    "            moves_base_power[i] = (\n",
    "                move.base_power / 100\n",
    "            )  # Simple rescaling to facilitate learning\n",
    "            if move.type:\n",
    "                moves_dmg_multiplier[i] = move.type.damage_multiplier(\n",
    "                    battle.opponent_active_pokemon.type_1,\n",
    "                    battle.opponent_active_pokemon.type_2,\n",
    "                    type_chart=type_chart\n",
    "                )\n",
    "\n",
    "        # We count how many pokemons have fainted in each team\n",
    "        fainted_mon_team = len([mon for mon in battle.team.values() if mon.fainted]) / 6\n",
    "        fainted_mon_opponent = (\n",
    "            len([mon for mon in battle.opponent_team.values() if mon.fainted]) / 6\n",
    "        )\n",
    "\n",
    "        # Final vector with 10 components\n",
    "        final_vector = np.concatenate(\n",
    "            [\n",
    "                moves_base_power,\n",
    "                moves_dmg_multiplier,\n",
    "                [fainted_mon_team, fainted_mon_opponent],\n",
    "            ]\n",
    "        )\n",
    "        return np.float32(final_vector)\n",
    "\n",
    "    def describe_embedding(self) -> Space:\n",
    "        low = [-1, -1, -1, -1, 0, 0, 0, 0, 0, 0]\n",
    "        high = [3, 3, 3, 3, 4, 4, 4, 4, 1, 1]\n",
    "        return Box(\n",
    "            np.array(low, dtype=np.float32),\n",
    "            np.array(high, dtype=np.float32),\n",
    "            dtype=np.float32,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the base environments to copy with make later.\n",
    "\n",
    "# First test the environment to ensure the class is consistent\n",
    "# with the OpenAI API\n",
    "# opponent = RandomPlayer(battle_format=\"gen8randombattle\")\n",
    "# test_env = SimpleRLPlayer(\n",
    "#     battle_format=\"gen8randombattle\", start_challenging=True, opponent=opponent\n",
    "# )\n",
    "# check_env(test_env)\n",
    "# test_env.close()\n",
    "\n",
    "# Create one environment for training and one for evaluation\n",
    "opponent = RandomPlayer(battle_format=\"gen8randombattle\")\n",
    "train_env_base = SimpleRLPlayer(\n",
    "    battle_format=\"gen8randombattle\", opponent=opponent, start_challenging=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openrl.envs.common import build_envs\n",
    "from typing import Callable, List, Optional, Union\n",
    "from gymnasium import Env\n",
    "import copy\n",
    "from openrl.configs.config import create_config_parser\n",
    "cfg_parser = create_config_parser()\n",
    "\n",
    "# register the base train environment in gymnasium.\n",
    "# register(id=\"SimpleRLPlayer/train_env\", entry_point=SimpleRLPlayer)\n",
    "# create custom make function to aid in setting up environments.\n",
    "def make_train_env(id, render_mode, disable_env_checker, **kwargs):\n",
    "    return train_env_base\n",
    "def make_train_envs(\n",
    "    id: str,\n",
    "    env_num: int = 1,\n",
    "    render_mode: Optional[Union[str, List[str]]] = None,\n",
    "    **kwargs,\n",
    ") -> List[Callable[[], Env]]:\n",
    "    env_wrappers = copy.copy(kwargs.pop(\"env_wrappers\", []))\n",
    "    env_fns = build_envs(\n",
    "        make=make_train_env,\n",
    "        id=id,\n",
    "        env_num=env_num,\n",
    "        render_mode=render_mode,\n",
    "        wrappers=env_wrappers,\n",
    "        **kwargs,\n",
    "    )\n",
    "    return env_fns\n",
    "\n",
    "train_envs = make(id=\"SimpleRLPlayer/train_env\", env_num=4, make_custom_envs = make_train_envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute dimensions\n",
    "# n_action = train_envs.action_space.n\n",
    "# input_shape = (1,) + train_envs.observation_space.shape\n",
    "\n",
    "# Create model\n",
    "# model = Sequential()\n",
    "# model.add(layers.Dense(128, activation=\"elu\", input_shape=input_shape))\n",
    "# model.add(layers.Flatten())\n",
    "# model.add(layers.Dense(64, activation=\"elu\"))\n",
    "# model.add(layers.Dense(n_action, activation=\"linear\"))\n",
    "\n",
    "# Defining the DQN\n",
    "# memory = SequentialMemory(limit=10000, window_length=1)\n",
    "\n",
    "# policy = LinearAnnealedPolicy(\n",
    "#     EpsGreedyQPolicy(),\n",
    "#     attr=\"eps\",\n",
    "#     value_max=1.0,\n",
    "#     value_min=0.05,\n",
    "#     value_test=0.0,\n",
    "#     nb_steps=10000,\n",
    "# )\n",
    "\n",
    "# dqn = DQNAgent(\n",
    "#     model=model,\n",
    "#     nb_actions=n_action,\n",
    "#     policy=policy,\n",
    "#     memory=memory,\n",
    "#     nb_steps_warmup=1000,\n",
    "#     gamma=0.5,\n",
    "#     target_model_update=1,\n",
    "#     delta_clip=0.01,\n",
    "#     enable_double_dqn=True,\n",
    "# )\n",
    "# dqn.compile(optimizers.Adam(learning_rate=0.00025), metrics=[\"mae\"])\n",
    "\n",
    "# TODO: Need to set config for dqn, here's the format:\n",
    "# https://github.com/OpenRL-Lab/openrl/blob/main/examples/cartpole/dqn_cartpole.yaml\n",
    "# https://github.com/OpenRL-Lab/openrl/blob/main/examples/cartpole/train_dqn_beta.py\n",
    "\n",
    "dqn_cfg = cfg_parser.parse_args([\"--config\", \"configs/basic_dqn.yaml\"])\n",
    "\n",
    "dqn_cfg.env = train_envs\n",
    "\n",
    "dqn_net = DQNNet(train_envs, cfg = dqn_cfg)\n",
    "\n",
    "dqn_agent = DQNAgent(dqn_net, env=train_envs)\n",
    "\n",
    "# Training the model\n",
    "dqn_agent.train(total_time_steps=10000)\n",
    "# dqn.fit(train_env, nb_steps=10000)\n",
    "# train_envs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "opponent = RandomPlayer(battle_format=\"gen8randombattle\")\n",
    "eval_env = SimpleRLPlayer(\n",
    "    battle_format=\"gen8randombattle\", opponent=opponent, start_challenging=True\n",
    ")\n",
    "\n",
    "print(\"Results against random player:\")\n",
    "dqn.test(eval_env, nb_episodes=100, verbose=False, visualize=False)\n",
    "print(\n",
    "    f\"DQN Evaluation: {eval_env.n_won_battles} victories out of {eval_env.n_finished_battles} episodes\"\n",
    ")\n",
    "second_opponent = MaxBasePowerPlayer(battle_format=\"gen8randombattle\")\n",
    "eval_env.reset_env(restart=True, opponent=second_opponent)\n",
    "print(\"Results against max base power player:\")\n",
    "dqn.test(eval_env, nb_episodes=100, verbose=False, visualize=False)\n",
    "print(\n",
    "    f\"DQN Evaluation: {eval_env.n_won_battles} victories out of {eval_env.n_finished_battles} episodes\"\n",
    ")\n",
    "eval_env.reset_env(restart=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the player with included util method\n",
    "n_challenges = 250\n",
    "placement_battles = 40\n",
    "eval_task = background_evaluate_player(\n",
    "    eval_env.agent, n_challenges, placement_battles\n",
    ")\n",
    "dqn.test(eval_env, nb_episodes=n_challenges, verbose=False, visualize=False)\n",
    "print(\"Evaluation with included method:\", eval_task.result())\n",
    "eval_env.reset_env(restart=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross evaluate the player with included util method\n",
    "n_challenges = 50\n",
    "players = [\n",
    "    eval_env.agent,\n",
    "    RandomPlayer(battle_format=\"gen8randombattle\"),\n",
    "    MaxBasePowerPlayer(battle_format=\"gen8randombattle\"),\n",
    "    SimpleHeuristicsPlayer(battle_format=\"gen8randombattle\"),\n",
    "]\n",
    "cross_eval_task = background_cross_evaluate(players, n_challenges)\n",
    "dqn.test(\n",
    "    eval_env,\n",
    "    nb_episodes=n_challenges * (len(players) - 1),\n",
    "    verbose=False,\n",
    "    visualize=False,\n",
    ")\n",
    "cross_evaluation = cross_eval_task.result()\n",
    "table = [[\"-\"] + [p.username for p in players]]\n",
    "for p_1, results in cross_evaluation.items():\n",
    "    table.append([p_1] + [cross_evaluation[p_1][p_2] for p_2 in results])\n",
    "print(\"Cross evaluation of DQN with baselines:\")\n",
    "print(tabulate(table))\n",
    "eval_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poke-rl-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
